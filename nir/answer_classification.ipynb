{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# НИР\n",
    "\n",
    "В работе дообучим модель bert-base-multilingual-cased для распознавания текста"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T04:59:11.023195Z",
     "start_time": "2024-06-22T04:59:09.580190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T04:59:14.304607Z",
     "start_time": "2024-06-22T04:59:14.031494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('data/train.json.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data')"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/train.json.zip'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mzipfile\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mzipfile\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mZipFile\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdata/train.json.zip\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m zip_ref:\n\u001B[1;32m      4\u001B[0m     zip_ref\u001B[38;5;241m.\u001B[39mextractall(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m/usr/lib/python3.11/zipfile.py:1294\u001B[0m, in \u001B[0;36mZipFile.__init__\u001B[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001B[0m\n\u001B[1;32m   1292\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m   1293\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1294\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp \u001B[38;5;241m=\u001B[39m io\u001B[38;5;241m.\u001B[39mopen(file, filemode)\n\u001B[1;32m   1295\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m:\n\u001B[1;32m   1296\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m filemode \u001B[38;5;129;01min\u001B[39;00m modeDict:\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data/train.json.zip'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T04:59:24.739533Z",
     "start_time": "2024-06-22T04:59:24.703560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = []\n",
    "with open('data/qas/combined_dataset_with_responses_and_classification.json', 'r') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            data.append(json.loads(line.strip()))\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "# Преобразование данных в DataFrame для удобства работы с ними\n",
    "df = pd.DataFrame(data)\n",
    "df.head(5)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                            Question Answer  \\\n",
       "0  Was Abraham Lincoln the sixteenth President of...    yes   \n",
       "1  Did Lincoln sign the National Banking Act of 1...    yes   \n",
       "2                   Did his mother die of pneumonia?     no   \n",
       "3  Did Lincoln beat John C. Breckinridge in the 1...    yes   \n",
       "4  Was Abraham Lincoln the first President of the...     No   \n",
       "\n",
       "                                       ModelResponse Classification  \n",
       "0  Yes, Abraham Lincoln was indeed the 16th Presi...            yes  \n",
       "1  No, Abraham Lincoln did not sign the National ...             no  \n",
       "2  I apologize, but there is no information provi...        neither  \n",
       "3  Actually, Abraham Lincoln did not face John C....             no  \n",
       "4  No, Abraham Lincoln was not the first Presiden...             no  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>ModelResponse</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Was Abraham Lincoln the sixteenth President of...</td>\n",
       "      <td>yes</td>\n",
       "      <td>Yes, Abraham Lincoln was indeed the 16th Presi...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Did Lincoln sign the National Banking Act of 1...</td>\n",
       "      <td>yes</td>\n",
       "      <td>No, Abraham Lincoln did not sign the National ...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Did his mother die of pneumonia?</td>\n",
       "      <td>no</td>\n",
       "      <td>I apologize, but there is no information provi...</td>\n",
       "      <td>neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Did Lincoln beat John C. Breckinridge in the 1...</td>\n",
       "      <td>yes</td>\n",
       "      <td>Actually, Abraham Lincoln did not face John C....</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Was Abraham Lincoln the first President of the...</td>\n",
       "      <td>No</td>\n",
       "      <td>No, Abraham Lincoln was not the first Presiden...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Функция предобработки текстовых данных"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T18:22:15.874548Z",
     "start_time": "2024-06-03T18:22:15.846634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_data(text_list, tokenizer, max_length=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text in text_list:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True, \n",
    "            max_length=max_length,\n",
    "            padding='max_length',  # Дополнение последовательности до максимальной длины\n",
    "            truncation=True,  # Усечение длинных последовательностей\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Подготовка данных к обучению и валидации\n",
    "Кодирование текста с помощью токенизатора BERT.\n",
    "Разделение данных на обучающую и валидационную выборки.\n",
    "Создание датасетов для обучения и валидации\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T18:22:19.519618Z",
     "start_time": "2024-06-03T18:22:15.875529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Загрузка предобученной модели и токенизатора BERT\n",
    "model_name = 'bert-base-multilingual-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3) # Настройка количества меток (классов)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Training on GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Training on CPU.\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Предобработка текстовых данных и подготовка входных данных для модели\n",
    "input_ids, attention_masks = preprocess_data(df['ModelResponse'].to_list(), tokenizer)\n",
    "# Преобразование меток в числовой формат\n",
    "labels = df['Classification'].apply(lambda x: 0 if x == 'neither' else 1 if x == 'yes' else 2).values\n",
    "labels = torch.tensor(labels).to(device)\n",
    "\n",
    "train_size = int(0.8 * len(df))\n",
    "val_size = len(df) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(TensorDataset(input_ids, attention_masks, labels), [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Training on GPU.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Функция для обучения модели"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T18:22:19.525629Z",
     "start_time": "2024-06-03T18:22:19.520604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(model, train_dataloader, val_dataloader, epochs=4):\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)  # Оптимизатор AdamW с заданной скоростью обучения\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Переключение модели в режим обучения\n",
    "        total_loss = 0  # Переменная для хранения общей потери\n",
    "\n",
    "        # Обучение на обучающей выборке\n",
    "        for batch in train_dataloader:\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            \n",
    "            model.zero_grad()  # Обнуление градиентов\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs.loss  # Получение значения потери\n",
    "            total_loss += loss.item()  # Добавление потери к общей сумме\n",
    "            loss.backward()  # Обратное распространение ошибки\n",
    "            optimizer.step()  # Обновление параметров модели\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "        print(f\"Training loss: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "        model.eval()  # Переключение модели в режим оценки\n",
    "        val_accuracy = 0  # Переменная для хранения общей точности на валидационной выборке\n",
    "        val_loss = 0  # Переменная для хранения общей потери на валидационной выборке\n",
    "\n",
    "        # Оценка на валидационной выборке\n",
    "        for batch in val_dataloader:\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "            loss = outputs.loss  # Получение значения потери\n",
    "            logits = outputs.logits  # Получение логитов\n",
    "            val_loss += loss.item()  # Добавление потери к общей сумме\n",
    "            preds = torch.argmax(logits, dim=1).flatten()  # Получение предсказаний\n",
    "            accuracy = (preds == b_labels).cpu().numpy().mean() * 100  # Вычисление точности\n",
    "            val_accuracy += accuracy\n",
    "\n",
    "        avg_val_accuracy = val_accuracy / len(val_dataloader)  # Средняя точность на валидационной выборке\n",
    "        avg_val_loss = val_loss / len(val_dataloader)  # Средняя потеря на валидационной выборке\n",
    "\n",
    "        # Вывод результатов текущей эпохи\n",
    "        print(f'Epoch {epoch+1}/{epochs}')\n",
    "        print(f'Training Loss: {avg_train_loss:.2f}')\n",
    "        print(f'Validation Loss: {avg_val_loss:.2f}')\n",
    "        print(f'Validation Accuracy: {avg_val_accuracy:.2f}%')\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Выполнение обучения модели"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T18:31:20.654311Z",
     "start_time": "2024-06-03T18:22:19.526610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train(model, train_dataloader, val_dataloader, epochs=20)\n",
    "\n",
    "model.save_pretrained('rusentiment_bert_model')\n",
    "tokenizer.save_pretrained('rusentiment_bert_model')\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denis/Documents/5_ITMO/deep-learning/deep-learning-practice/venv/lib/python3.11/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6075519683305174\n",
      "Epoch 1/20\n",
      "Training Loss: 0.61\n",
      "Validation Loss: 0.33\n",
      "Validation Accuracy: 87.81%\n",
      "Training loss: 0.305311486730352\n",
      "Epoch 2/20\n",
      "Training Loss: 0.31\n",
      "Validation Loss: 0.32\n",
      "Validation Accuracy: 88.44%\n",
      "Training loss: 0.22287968659074978\n",
      "Epoch 3/20\n",
      "Training Loss: 0.22\n",
      "Validation Loss: 0.31\n",
      "Validation Accuracy: 88.12%\n",
      "Training loss: 0.1720857791369781\n",
      "Epoch 4/20\n",
      "Training Loss: 0.17\n",
      "Validation Loss: 0.41\n",
      "Validation Accuracy: 87.50%\n",
      "Training loss: 0.12582770636072382\n",
      "Epoch 5/20\n",
      "Training Loss: 0.13\n",
      "Validation Loss: 0.44\n",
      "Validation Accuracy: 88.37%\n",
      "Training loss: 0.13034197927918284\n",
      "Epoch 6/20\n",
      "Training Loss: 0.13\n",
      "Validation Loss: 0.46\n",
      "Validation Accuracy: 87.19%\n",
      "Training loss: 0.05664774967881385\n",
      "Epoch 7/20\n",
      "Training Loss: 0.06\n",
      "Validation Loss: 0.54\n",
      "Validation Accuracy: 84.93%\n",
      "Training loss: 0.05183425411814824\n",
      "Epoch 8/20\n",
      "Training Loss: 0.05\n",
      "Validation Loss: 0.56\n",
      "Validation Accuracy: 87.81%\n",
      "Training loss: 0.06156612955091987\n",
      "Epoch 9/20\n",
      "Training Loss: 0.06\n",
      "Validation Loss: 0.58\n",
      "Validation Accuracy: 88.37%\n",
      "Training loss: 0.06277498389536049\n",
      "Epoch 10/20\n",
      "Training Loss: 0.06\n",
      "Validation Loss: 0.59\n",
      "Validation Accuracy: 86.88%\n",
      "Training loss: 0.06640525770490058\n",
      "Epoch 11/20\n",
      "Training Loss: 0.07\n",
      "Validation Loss: 0.49\n",
      "Validation Accuracy: 88.44%\n",
      "Training loss: 0.024166057257389184\n",
      "Epoch 12/20\n",
      "Training Loss: 0.02\n",
      "Validation Loss: 0.55\n",
      "Validation Accuracy: 87.50%\n",
      "Training loss: 0.013793021327001043\n",
      "Epoch 13/20\n",
      "Training Loss: 0.01\n",
      "Validation Loss: 0.62\n",
      "Validation Accuracy: 87.81%\n",
      "Training loss: 0.00945801259658765\n",
      "Epoch 14/20\n",
      "Training Loss: 0.01\n",
      "Validation Loss: 0.62\n",
      "Validation Accuracy: 87.50%\n",
      "Training loss: 0.003631945129018277\n",
      "Epoch 15/20\n",
      "Training Loss: 0.00\n",
      "Validation Loss: 0.68\n",
      "Validation Accuracy: 87.19%\n",
      "Training loss: 0.02414481745217927\n",
      "Epoch 16/20\n",
      "Training Loss: 0.02\n",
      "Validation Loss: 0.61\n",
      "Validation Accuracy: 87.19%\n",
      "Training loss: 0.017735426875879055\n",
      "Epoch 17/20\n",
      "Training Loss: 0.02\n",
      "Validation Loss: 0.64\n",
      "Validation Accuracy: 86.25%\n",
      "Training loss: 0.012336415527533973\n",
      "Epoch 18/20\n",
      "Training Loss: 0.01\n",
      "Validation Loss: 0.66\n",
      "Validation Accuracy: 86.88%\n",
      "Training loss: 0.0032055696963652737\n",
      "Epoch 19/20\n",
      "Training Loss: 0.00\n",
      "Validation Loss: 0.67\n",
      "Validation Accuracy: 88.12%\n",
      "Training loss: 0.0009395783385116374\n",
      "Epoch 20/20\n",
      "Training Loss: 0.00\n",
      "Validation Loss: 0.72\n",
      "Validation Accuracy: 88.44%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('rusentiment_bert_model/tokenizer_config.json',\n",
       " 'rusentiment_bert_model/special_tokens_map.json',\n",
       " 'rusentiment_bert_model/vocab.txt',\n",
       " 'rusentiment_bert_model/added_tokens.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Функция для предсказания класса текста"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T18:31:20.658151Z",
     "start_time": "2024-06-03T18:31:20.655102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict(text, model, tokenizer):\n",
    "    model.eval()  # Переключение модели в режим оценки\n",
    "    inputs = tokenizer.encode_plus(text, return_tensors='pt', max_length=128, truncation=True, padding='max_length')\n",
    "    input_ids = inputs['input_ids'].to(device)  # Получение идентификаторов токенов\n",
    "    attention_mask = inputs['attention_mask'].to(device)  # Получение масок внимания\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)  # Получение выходов модели\n",
    "        logits = outputs.logits  # Получение логитов\n",
    "        predicted_class = torch.argmax(logits, dim=1).item()  # Определение предсказанного класса\n",
    "\n",
    "    return predicted_class"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "# Тестирование функции предсказания на примере текста\n",
    "test_text = \"No\"\n",
    "prediction = predict(test_text, model, tokenizer)\n",
    "print(f'Text: {test_text}')\n",
    "print(f'Prediction: {prediction} {\"Neither\" if prediction == 0 else \"Yes\" if prediction == 1 else \"No\"}')\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YD9G_j740exk",
    "outputId": "20b8456f-7fbf-498b-f4f8-6d6390222575",
    "ExecuteTime": {
     "end_time": "2024-06-03T18:31:20.694845Z",
     "start_time": "2024-06-03T18:31:20.658935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: No\n",
      "Prediction: 2 No\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T18:31:20.706364Z",
     "start_time": "2024-06-03T18:31:20.696152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_text = \"\"\"\n",
    "No, Anders Celsius was not born in Uppsala, Sweden. He was actually born on November 27, 1701, in Uppsala, Sweden, but he spent most of his life in the southern part of Sweden.\n",
    "\"\"\"\n",
    "prediction = predict(test_text, model, tokenizer)\n",
    "print(f'Text: {test_text}')\n",
    "print(f'Prediction: {prediction} {\"Neither\" if prediction == 0 else \"Yes\" if prediction == 1 else \"No\"}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: \n",
      "No, Anders Celsius was not born in Uppsala, Sweden. He was actually born on November 27, 1701, in Uppsala, Sweden, but he spent most of his life in the southern part of Sweden.\n",
      "\n",
      "Prediction: 2 No\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "test_text = \"\"\"\n",
    "I apologize, but there isn't enough information provided to answer this question accurately. Could you please provide more context or clarify which \"his father\" you are referring to?\n",
    "\"\"\"\n",
    "prediction = predict(test_text, model, tokenizer)\n",
    "print(f'Text: {test_text}')\n",
    "print(f'Prediction: {prediction} {\"Neither\" if prediction == 0 else \"Yes\" if prediction == 1 else \"No\"}')"
   ],
   "metadata": {
    "id": "W4d8jn0ZLFeh",
    "ExecuteTime": {
     "end_time": "2024-06-03T18:31:20.724303Z",
     "start_time": "2024-06-03T18:31:20.707244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: \n",
      "I apologize, but there isn't enough information provided to answer this question accurately. Could you please provide more context or clarify which \"his father\" you are referring to?\n",
      "\n",
      "Prediction: 0 Neither\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T18:31:20.736325Z",
     "start_time": "2024-06-03T18:31:20.725089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_text = \"\"\"\n",
    "The modern hi-hat, also known as the cymbal pair, did not directly evolve from clash cymbals. Clash cymbals were used in the early days of drumming to create a loud, sharp sound by striking two cymbals together.\n",
    "\n",
    "The modern hi-hat, on the other hand, is a specific type of cymbal pair that consists of two cymbals mounted on a pedal-controlled mechanism. This design allows for the cymbals to be opened and closed quickly, creating a distinctive \"chick\" sound.\n",
    "\n",
    "While clash cymbals were used in early drumming, the modern hi-hat as we know it today was developed later, in the mid-20th century. The first patent for a pedal-controlled cymbal pair was granted to William F. Ludwig Sr. in 1939, and his company, W.F.Ludwig & Co., began manufacturing and selling the design.\n",
    "\n",
    "So while clash cymbals may have influenced the development of drumming techniques and sounds, they did not directly contribute to the evolution of the modern hi-hat as we know it today.\n",
    "\"\"\"\n",
    "prediction = predict(test_text, model, tokenizer)\n",
    "print(f'Text: {test_text}')\n",
    "print(f'Prediction: {prediction} {\"Neither\" if prediction == 0 else \"Yes\" if prediction == 1 else \"No\"}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: \n",
      "The modern hi-hat, also known as the cymbal pair, did not directly evolve from clash cymbals. Clash cymbals were used in the early days of drumming to create a loud, sharp sound by striking two cymbals together.\n",
      "\n",
      "The modern hi-hat, on the other hand, is a specific type of cymbal pair that consists of two cymbals mounted on a pedal-controlled mechanism. This design allows for the cymbals to be opened and closed quickly, creating a distinctive \"chick\" sound.\n",
      "\n",
      "While clash cymbals were used in early drumming, the modern hi-hat as we know it today was developed later, in the mid-20th century. The first patent for a pedal-controlled cymbal pair was granted to William F. Ludwig Sr. in 1939, and his company, W.F.Ludwig & Co., began manufacturing and selling the design.\n",
      "\n",
      "So while clash cymbals may have influenced the development of drumming techniques and sounds, they did not directly contribute to the evolution of the modern hi-hat as we know it today.\n",
      "\n",
      "Prediction: 2 No\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T18:31:20.750901Z",
     "start_time": "2024-06-03T18:31:20.736989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_text = \"\"\"\n",
    "The question implies that we are discussing someone who made a significant discovery or created something notable. To answer your question, yes, many scientists and researchers publish their work to share it with others in the field and make it accessible for further study and potential applications.\n",
    "\n",
    "Publishing research involves submitting an article or paper to a reputable journal or conference proceedings, where it undergoes peer review before being accepted for publication. This process helps ensure that the work is of high quality, accurate, and contributes meaningfully to the existing body of knowledge.\n",
    "\n",
    "In addition to publishing in traditional journals, researchers may also share their findings through online platforms, blogs, or social media. Some may present their work at conferences or workshops, either in person or virtually.\"\"\"\n",
    "prediction = predict(test_text, model, tokenizer)\n",
    "print(f'Text: {test_text}')\n",
    "print(f'Prediction: {prediction} {\"Neither\" if prediction == 0 else \"Yes\" if prediction == 1 else \"No\"}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: \n",
      "The question implies that we are discussing someone who made a significant discovery or created something notable. To answer your question, yes, many scientists and researchers publish their work to share it with others in the field and make it accessible for further study and potential applications.\n",
      "\n",
      "Publishing research involves submitting an article or paper to a reputable journal or conference proceedings, where it undergoes peer review before being accepted for publication. This process helps ensure that the work is of high quality, accurate, and contributes meaningfully to the existing body of knowledge.\n",
      "\n",
      "In addition to publishing in traditional journals, researchers may also share their findings through online platforms, blogs, or social media. Some may present their work at conferences or workshops, either in person or virtually.\n",
      "Prediction: 1 Yes\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T18:31:20.752958Z",
     "start_time": "2024-06-03T18:31:20.751480Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 13
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d43fbba",
   "metadata": {
    "cellId": "6tgifddr0g4wmt8cy6lx7h"
   },
   "source": [
    "# **Seminar 2 - Attention и Transformer**\n",
    "*Naumov Anton (Any0019)*\n",
    "\n",
    "*To contact me in telegram: @any0019*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d08d4a",
   "metadata": {
    "cellId": "l5w7dxjwy9qp98m1xhmr",
    "tags": []
   },
   "source": [
    "## 1. Построим Transformer с нуля в Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503544cb-b0c7-45db-946a-61cb2fdd57d2",
   "metadata": {},
   "source": [
    "<img src=\"https://i.pinimg.com/736x/c1/83/1a/c1831a58ecc935fc0f2ef4d35ce4fddb.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3ae293-0776-4685-ad45-f5c0ad517f6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a9d7b3-71f0-4528-8def-47578906fa1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1 Multi-head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caa6d91-f5ef-4cfe-a96b-29970e28ffaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Основной класс - MultiHeadAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6e24c6-dc8d-466d-bd12-f25389032aa5",
   "metadata": {},
   "source": [
    "**Инициализация:**\n",
    "* _in_size_ ~ размер эмбеддингов на входе\n",
    "* _head_size_ ~ размер эмбеддингов матриц Q, K, V после преобразования\n",
    "* _num_heads_ ~ число голов\n",
    "* _out_size_ ~ размер эмбеддингов на выходе\n",
    "* _query_in_size_ ~ размер эмбеддингов на входе для query (если None, то in_size)\n",
    "\n",
    "**Forward:**\n",
    "* _query, key, value_ ~ 3 тензора (по одному под Q, K и V преобразования - это ещё не сами тензоры $\\text{batch_size} \\times seq \\times d_k$, а тензоры $\\text{batch_size} \\times seq \\times \\text{in_size}$)\n",
    "* _mask_ ~ булева маска для Masked Multi-head Attention (в декодере)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58ac31c-1b8e-40ad-9b06-5648be5330d8",
   "metadata": {},
   "source": [
    "$$ Attention(Q, K, V) = softmax\\Bigg(\\frac{QK^T}{\\sqrt{d_k}}\\Bigg) \\cdot V $$\n",
    "$$ MultiHead(Q, K, V) = Concat(head_1, ..., head_H) \\cdot W^O \\quad ; \\quad head_i = Attention(Q W_i^Q, K W_i^K, V W_i^V)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f06172f-89e6-460e-96b0-c713be41b67c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Class to calculate Multi-head attention (or Masked Multi-head attention for decoder) operation\n",
    "    \"\"\"\n",
    "    def __init__(self, in_size, head_size, num_heads, out_size, query_in_size=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_size: embedding size of input\n",
    "            head_size: hidden size of Q, K, V matrices\n",
    "            num_heads: number of heads\n",
    "            out_size: output embedding size\n",
    "            query_in_size: embedding size of input for query (if not provided - same as in_size)\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        # Запишем все переданые гиперпараметры слоя\n",
    "        self.in_size = in_size\n",
    "        self.head_size = head_size\n",
    "        self.num_heads = num_heads\n",
    "        self.out_size = out_size\n",
    "        self.query_in_size = self.in_size if query_in_size is None else query_in_size\n",
    "       \n",
    "        # Линейные преобразования для Q, K, V матриц (сразу хотим получить все Q, K, V матрицы)\n",
    "        self.query_matrix = ...\n",
    "        self.key_matrix = ...\n",
    "        self.value_matrix = ...\n",
    "        \n",
    "        # Линейное преобразование для получения выхода после конкатенации голов\n",
    "        self.out = ...\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "           query : tensor for query\n",
    "           key : tensor for key\n",
    "           value : tensor for value\n",
    "           mask: mask for decoder\n",
    "        \n",
    "        Returns:\n",
    "           output vector from multihead attention\n",
    "        \"\"\"\n",
    "        # Тензоры приходят размера batch_size x seq_len x in_size\n",
    "        batch_size = key.size(0)\n",
    "        seq_len = key.size(1)\n",
    "        \n",
    "        # Число токенов в query будет другим для decoder-а\n",
    "        query_seq_len = query.size(1)\n",
    "       \n",
    "        # Применяем линейные преобразования на входе\n",
    "        q = ...\n",
    "        k = ...\n",
    "        v = ...\n",
    "       \n",
    "        # Считаем релевантность\n",
    "        relevance = ...\n",
    "        \n",
    "        # Если есть маска (для декодера), то заполняем значения по маске как минус бесконечность (чтобы exp(r) = 0 в softmax)\n",
    "        if mask is not None:\n",
    "             relevance = ...\n",
    "\n",
    "        # Получаем вероятности\n",
    "        relevance = ...\n",
    " \n",
    "        # Считаем выходы из каждой головы\n",
    "        head_i = ...\n",
    "        \n",
    "        # Конкатенируем выходы\n",
    "        concat = ...\n",
    "        \n",
    "        # Финальное линейное преобразование\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24742a18-9f98-4c77-b91a-1b3dada2a2b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Протестируем MultiHeadAttention для энкодера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebd7342-1a4a-4aa7-821f-ec1e145e0e0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_layer = MultiHeadAttention(\n",
    "    in_size=10,\n",
    "    head_size=4,\n",
    "    num_heads=3,\n",
    "    out_size=15,\n",
    ")\n",
    "\n",
    "tmp_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f01b89-df85-4cdd-bee3-73e5985301e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Проверяем в обычном прямом проходе из энкодера\n",
    "tmp_input = torch.rand(2, 5, 10)\n",
    "\n",
    "print(\"Encoder-like input, no mask\")\n",
    "print(f'Input shape: {tmp_input.shape}')\n",
    "tmp_output = tmp_layer(tmp_input, tmp_input, tmp_input)\n",
    "print(f'Output shape: {tmp_output.shape}')\n",
    "\n",
    "del tmp_input, tmp_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af1baf8-8839-4306-a99d-f1f92d49f4ff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Протестируем MultiHeadAttention для смеси энкодера и декодера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdca3c7-b1da-4ed1-acb5-a4b80c8c31cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_layer = MultiHeadAttention(\n",
    "    in_size=10,\n",
    "    head_size=4,\n",
    "    num_heads=3,\n",
    "    out_size=15,\n",
    "    query_in_size=12,\n",
    ")\n",
    "\n",
    "tmp_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7d05e6-93f3-4b88-b2a0-fda90c3b7a13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Проверяем в прямом проходе из декодера, где мы смешиваем информацию из энкодера и декодера\n",
    "tmp_input_q = torch.rand(2, 5, 12)\n",
    "tmp_input_kv = torch.rand(2, 7, 10)\n",
    "\n",
    "print(\"Encoder+Decoder-like input, no mask\")\n",
    "print(f'Input Q shape: {tmp_input_q.shape}')\n",
    "print(f'Input KV shape: {tmp_input_kv.shape}')\n",
    "\n",
    "tmp_output = tmp_layer(tmp_input_q, tmp_input_kv, tmp_input_kv)\n",
    "print(f'Output shape: {tmp_output.shape}')\n",
    "\n",
    "del tmp_input_q, tmp_input_kv, tmp_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e262a1-f8d1-445f-9ec0-622d11e12ec9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Треугольная маска в декодере"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f0a70b-0d8d-49ab-9796-776a689adae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_decoder_mask(decoder_embed):\n",
    "    \"\"\"\n",
    "    Make mask for decoder Masked Multi-head Attention based on input sequence\n",
    "    Args:\n",
    "        decoder_embed: decoder sequence after embed\n",
    "    Returns:\n",
    "        mask: mask for Masked Multi-head Attention\n",
    "    \"\"\"\n",
    "    batch_size, decoder_seq_len, _ = decoder_embed.shape\n",
    "    mask = torch.tril(torch.ones((decoder_seq_len, decoder_seq_len))).expand(\n",
    "        batch_size, 1, decoder_seq_len, decoder_seq_len\n",
    "    ).bool()\n",
    "    return mask "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dbb866-7989-4310-9aa7-b1c2482b5d68",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Протестируем MultiHeadAttention для декодера с маской"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcfc9ae-0488-4c0b-9071-7419ea1b4e2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_input = torch.rand(1, 10, 256)\n",
    "tmp_mask = make_decoder_mask(tmp_input)\n",
    "print(f\"Mask shape: {tmp_mask.shape}\")\n",
    "\n",
    "# Визуализируем\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plt.imshow(tmp_mask[0, 0, :, :])\n",
    "# Добавляем текстовые надписи\n",
    "for i in range(tmp_mask.shape[-2]):\n",
    "    for j in range(tmp_mask.shape[-1]):\n",
    "        text = plt.text(j, i, tmp_mask[0, 0, i, j].item(), ha=\"center\", va=\"center\", color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb5bc0c-d3bb-472a-89c6-58133a20b2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_layer = MultiHeadAttention(\n",
    "    in_size=10,\n",
    "    head_size=4,\n",
    "    num_heads=3,\n",
    "    out_size=15,\n",
    ")\n",
    "\n",
    "tmp_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58408a1-d3b6-4b11-af17-2c31a0918c95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_input = torch.rand(2, 5, 10)\n",
    "tmp_mask = make_decoder_mask(tmp_input)\n",
    "\n",
    "print(\"Decoder-like input, with mask\")\n",
    "print(f'Input shape: {tmp_input.shape}')\n",
    "print(f'Mask shape: {tmp_mask.shape}')\n",
    "\n",
    "tmp_output = tmp_layer(tmp_input, tmp_input, tmp_input, tmp_mask)\n",
    "print(f'Output shape: {tmp_output.shape}')\n",
    "\n",
    "del tmp_input, tmp_mask, tmp_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890c5767-3b13-47f9-8c96-c5e1907436fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.2 Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969e21bb-03f7-4128-9e9c-e03896ef685e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Основной класс - PositionalEncoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0de7a4-6534-40fd-a1ae-bfee9ab0b946",
   "metadata": {},
   "source": [
    "**Инициализация:**\n",
    "* _max_seq_len_ ~ максимальный размер в токенах последовательности\n",
    "* _emb_size_ ~ размер эмбеддингов на входе\n",
    "\n",
    "**Forward:**\n",
    "* _decoder_emb_ ~ эмбеддинги токенов из входа декодера"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3695c6b5-05a8-416b-8ad9-1d0981f1ed88",
   "metadata": {},
   "source": [
    "$$\\text{PE}_{(\\text{pos}, 2i)} = sin\\Bigg( \\frac{\\text{pos}}{10000^{\\frac{2i}{\\text{emb_size}}}} \\Bigg) \\quad ; \\quad \\text{PE}_{(\\text{pos}, 2i + 1)} = cos\\Bigg( \\frac{\\text{pos}}{10000^{\\frac{2i}{\\text{emb_size}}}} \\Bigg)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db772e78-b56e-4a80-9fc7-46bba533e0f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Class to calculate Positional Encodings, suggested in `Attention is all you need [Vaswaniet al., 2017]`\n",
    "    \"\"\"\n",
    "    def __init__(self, max_seq_len, emb_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_seq_len: max length of input sequence\n",
    "            emb_size: demension of embedding\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Запишем все переданые гиперпараметры слоя\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.emb_size = emb_size\n",
    "        \n",
    "        # Посчитаем позиционные эмбеддинги в тензорном виде\n",
    "        ...\n",
    "        pe = ...\n",
    "        \n",
    "        # Добавляем полученный тензор как параметр, который будет сохранятся вместе с моделью, но не будет обучаться\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "\n",
    "    def forward(self, decoder_emb):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            decoder_emb: decoder sequence after embed\n",
    "        Returns:\n",
    "            output: input with positional encodings\n",
    "        \"\"\"\n",
    "        # Тензоры приходят размера batch_size x seq_len x emb_size\n",
    "        seq_len = decoder_emb.size(1)\n",
    "        \n",
    "        # Прибавляем позиционные эмбеддинги\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823dc20a-22a4-4c73-bfe6-4a40646316ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Протестируем PositionalEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c516f14-54e8-4915-9b05-7343cc0c4791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_layer = PositionalEncoding(\n",
    "    max_seq_len=5,\n",
    "    emb_size=10,\n",
    ")\n",
    "\n",
    "tmp_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defb8a45-520e-4ac7-bad1-c5e3a017906d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_input = torch.rand(2, 5, 10)\n",
    "\n",
    "print(f'Input shape: {tmp_input.shape}')\n",
    "tmp_output = tmp_layer(tmp_input)\n",
    "print(f'Output shape: {tmp_output.shape}')\n",
    "\n",
    "del tmp_input, tmp_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fb4dad-a30c-429d-bbaa-3b073a94ceb4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Посмотрим на позиционные эмбеддинги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01516cb-c9b8-449e-8ee4-17af03cdc653",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_layer.pe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce117edf-edb1-4e74-a500-aa6cdea7cdbe",
   "metadata": {},
   "source": [
    "Обоснование из статьи [Attention is all you need [Vaswaniet al., 2017]](https://www.semanticscholar.org/reader/204e3073870fae3d05bcbc2f6a8e263d9b72e776):\n",
    "\n",
    "We chose this function because we hypothesized it would allow the model to easily learn to attend by\n",
    "relative positions, since for any fixed offset k, $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec6cfd5-5de5-48f5-b688-f90f4ab01121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_layer = PositionalEncoding(\n",
    "    max_seq_len=200,\n",
    "    emb_size=100,\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plt.imshow(tmp_layer.pe[0, :, :], aspect=\"auto\")\n",
    "plt.xlabel(\"emb_size\")\n",
    "plt.ylabel(\"max_seq_len\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e289d87-ef49-44f2-83d7-3731351e5bb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plt.imshow(tmp_layer.pe[0, :, 50:], aspect=\"auto\")\n",
    "plt.xlabel(\"emb_size\")\n",
    "plt.ylabel(\"max_seq_len\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bf74e1-635f-4c06-8319-2e8bc9f2aa8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plt.imshow(tmp_layer.pe[0, :, 50:51], aspect=\"auto\")\n",
    "plt.xlabel(\"emb_size\")\n",
    "plt.ylabel(\"max_seq_len\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5cfeb2-8885-4548-80e0-a6329f7b476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plt.imshow(tmp_layer.pe[0, 50:51, :], aspect=\"auto\")\n",
    "plt.xlabel(\"emb_size\")\n",
    "plt.ylabel(\"max_seq_len\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad55aa40-2f30-4046-8e2c-be40f63bb6b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.3 Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a562d75-ee57-42b0-b0d4-782bae9ef7a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Картинка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a5a444-d980-4bd1-8c1a-ef88e9ae3aba",
   "metadata": {},
   "source": [
    "<img src=\"https://www.researchgate.net/profile/Ehsan-Amjadian/publication/352239001/figure/fig1/AS:1033334390013952@1623377525434/Detailed-view-of-a-transformer-encoder-block-It-first-passes-the-input-through-an.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a244101-f865-42a7-b228-1c2f594f9608",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### TransformerEncoderBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3eee77-d1b8-4755-bf24-6cdf44e29fc6",
   "metadata": {},
   "source": [
    "**Инициализация:**\n",
    "* _in_size_ ~ размер эмбеддингов на входе\n",
    "* _head_size_ ~ размер эмбеддингов матриц Q, K, V после преобразования\n",
    "* _num_heads_ ~ число голов attention\n",
    "* _out_size_ ~ размер эмбеддингов на выходе attention и блока\n",
    "* _ff_hidden_size_ ~ размер скрытого представления для линейных слоёв\n",
    "* _dropout_p_ ~ вероятность для dropout-ов\n",
    "* _query_in_size_ ~ размер эмбеддингов на входе для query (если None, то in_size)\n",
    "\n",
    "**Forward:**\n",
    "* _query, key, value_ ~ 3 тензора (по одному под Q, K и V преобразования - это ещё не сами тензоры $\\text{batch_size} \\times seq \\times d_k$, а тензоры $\\text{batch_size} \\times seq \\times \\text{in_size}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf31e8a-12ef-4a61-abf0-fc2f539f6a26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Class with one full block within transformer's encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, in_size, head_size, num_heads, out_size, ff_hidden_size, dropout_p=0.2, query_in_size=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "           in_size: input embedding size\n",
    "           head_size: size of each attention head\n",
    "           num_heads: number of attention heads\n",
    "           out_size: output embedding size\n",
    "           ff_hidden_size: hidden size for feed forward net\n",
    "           dropout_p: probability for dropout\n",
    "           query_in_size: embedding size of input for query (if not provided - same as in_size)\n",
    "        \"\"\"\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        \n",
    "        # Запишем все переданые гиперпараметры слоя\n",
    "        self.in_size = in_size\n",
    "        self.head_size = head_size\n",
    "        self.num_heads = num_heads\n",
    "        self.out_size = out_size\n",
    "        self.ff_hidden_size = ff_hidden_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.query_in_size = in_size if query_in_size is None else query_in_size\n",
    "        \n",
    "        self.attention = ...\n",
    "        # Если выход и вход attention-а имеют разный размер, то используем линейный слой на residual connection-е\n",
    "        self.adapt_residual = ...\n",
    "        \n",
    "        self.norm_1 = ...\n",
    "        self.dropout_1 = ...\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(OrderedDict([\n",
    "            (\"lin_1\", ...),\n",
    "            (\"act\", ...),\n",
    "            (\"lin_2\", ...),\n",
    "        ]))\n",
    "\n",
    "        self.norm_2 = ...\n",
    "        self.dropout_2 = ...\n",
    "        \n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "           block_input: input to corresponding block\n",
    "        \"\"\"\n",
    "        # Получаем на вход 3 тензора batch_size x seq_len x in_size\n",
    "        attention_out = ...\n",
    "        attention_residual_out = ...\n",
    "        norm_1_out = ...\n",
    "\n",
    "        ff_out = ...\n",
    "        ff_residual_out = ...\n",
    "        norm_2_out = ...\n",
    "        return norm_2_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e653ddc4-6b5b-444b-8545-678539f98659",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Протестируем TransformerEncoderBlock для энкодера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e1e25e-0f4a-4ab2-9dfc-bf6cf96be7c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Проверяем в обычном прямом проходе из энкодера\n",
    "tmp_layer = TransformerEncoderBlock(\n",
    "    in_size=10,\n",
    "    head_size=7,\n",
    "    num_heads=2,\n",
    "    out_size=15,\n",
    "    ff_hidden_size=20,\n",
    "    dropout_p=0.1,\n",
    ")\n",
    "\n",
    "tmp_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1c02b6-764b-4591-91bf-5ad313bcfc5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_input = torch.rand(2, 5, 10)\n",
    "\n",
    "print(\"Encoder-like input\")\n",
    "print(f'Input shape: {tmp_input.shape}')\n",
    "tmp_output = tmp_layer(tmp_input, tmp_input, tmp_input)\n",
    "print(f'Output shape: {tmp_output.shape}')\n",
    "\n",
    "del tmp_input, tmp_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d2404a-1fdb-4470-a66c-5cdba29a8790",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Протестируем TransformerEncoderBlock для декодера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82f323e-0c0f-46fd-9a50-dd30d9f9fdee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_layer = TransformerEncoderBlock(\n",
    "    in_size=10,\n",
    "    head_size=7,\n",
    "    num_heads=2,\n",
    "    out_size=15,\n",
    "    ff_hidden_size=20,\n",
    "    dropout_p=0.1,\n",
    "    query_in_size=12,\n",
    ")\n",
    "\n",
    "tmp_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3446cec-0b5a-46b7-b821-ea5a969d37fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Проверяем в прямом проходе из декодера, где мы смешиваем информацию из энкодера и декодера\n",
    "tmp_input_q = torch.rand(2, 5, 12)\n",
    "tmp_input_kv = torch.rand(2, 7, 10)\n",
    "\n",
    "print(\"Encoder+Decoder-like input\")\n",
    "print(f'Input Q shape: {tmp_input_q.shape}')\n",
    "print(f'Input KV shape: {tmp_input_kv.shape}')\n",
    "\n",
    "tmp_output = tmp_layer(tmp_input_q, tmp_input_kv, tmp_input_kv)\n",
    "print(f'Output shape: {tmp_output.shape}')\n",
    "\n",
    "del tmp_input_q, tmp_input_kv, tmp_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cf9c2b-fd58-41ce-9d75-d799aaf8ffaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-20T11:40:51.214657Z",
     "iopub.status.busy": "2024-02-20T11:40:51.213938Z",
     "iopub.status.idle": "2024-02-20T11:40:51.236800Z",
     "shell.execute_reply": "2024-02-20T11:40:51.235622Z",
     "shell.execute_reply.started": "2024-02-20T11:40:51.214610Z"
    },
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### TransformerEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b57aeb4-9853-49cd-9d0d-4d7c1151d27f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-20T11:40:51.214657Z",
     "iopub.status.busy": "2024-02-20T11:40:51.213938Z",
     "iopub.status.idle": "2024-02-20T11:40:51.236800Z",
     "shell.execute_reply": "2024-02-20T11:40:51.235622Z",
     "shell.execute_reply.started": "2024-02-20T11:40:51.214610Z"
    },
    "tags": []
   },
   "source": [
    "**Инициализация:**\n",
    "* _max_seq_len_ ~ максимальный размер в токенах последовательности\n",
    "* _vocab_size_ ~ размер словаря\n",
    "* _emb_size_ ~ размер эмбеддингов на входе\n",
    "* _num_layers_ ~ число TransformerEncoderBlock-ов\n",
    "* _att_out_size_ ~ размер эмбеддингов на выходе attention и блока\n",
    "* _att_head_size_ ~ размер эмбеддингов матриц Q, K, V после преобразования\n",
    "* _num_heads_ ~ число голов attention\n",
    "* _ff_hidden_size_ ~ размер скрытого представления для линейных слоёв\n",
    "* _dropout_p_ ~ вероятность для dropout-ов\n",
    "\n",
    "**Forward:**\n",
    "* _encoder_input_ ~ токены входа в энкодер до эмбеддингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8222586-c33e-491f-a4ca-0ed8f0604bf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Class for encoder within transformer.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_seq_len, vocab_size, emb_size, num_layers, att_out_size, att_head_size, num_heads, ff_hidden_size, dropout_p):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_seq_len : maximum length of input sequence\n",
    "            vocab_size: size of the vocabulary\n",
    "            emb_size: embeddings size\n",
    "            num_layers: number of encoder layers\n",
    "            att_out_size: output size for attention and each encoder block\n",
    "            att_head_size: size of each attention head\n",
    "            num_heads: number of heads in multihead attention\n",
    "            ff_hidden_size: hidden size for feed forward net\n",
    "            dropout_p: probability for dropout\n",
    "        \"\"\"\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        # Запишем все переданые гиперпараметры слоя\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.num_layers = num_layers\n",
    "        self.att_out_size = att_out_size\n",
    "        self.att_head_size = att_head_size\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_hidden_size = ff_hidden_size\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        self.embedding_layer = nn.Embedding(self.vocab_size, self.emb_size)\n",
    "        self.positional_encoder = PositionalEncoding(self.max_seq_len, self.emb_size)\n",
    "\n",
    "        # Красивая обёртка для модулей в dict\n",
    "        self.encoder_blocks = nn.ModuleDict({\n",
    "            f\"encoder_block_{i}\": TransformerEncoderBlock(\n",
    "                in_size=self.emb_size if i==0 else self.att_out_size,\n",
    "                head_size=self.att_head_size,\n",
    "                num_heads=self.num_heads,\n",
    "                out_size=self.att_out_size,\n",
    "                ff_hidden_size=self.ff_hidden_size,\n",
    "                dropout_p=self.dropout_p,\n",
    "            ) for i in range(self.num_layers)\n",
    "        })\n",
    "    \n",
    "    def forward(self, encoder_input):\n",
    "        # Получаем на вход batch_size x seq_len\n",
    "        encoder_emb = self.embedding_layer(encoder_input)  # (batch_size, seq_len, emb_size)\n",
    "        out = self.positional_encoder(encoder_emb)\n",
    "        for block in self.encoder_blocks.values():\n",
    "            out = block(out, out, out)  # (batch_size, seq_len, att_out_size)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec5d06-59ce-4a6f-a7de-f77b1b660fba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Протестируем TransformerEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03227809-158a-414b-98d8-e63c185e4d41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_layer = TransformerEncoder(\n",
    "    max_seq_len=20,\n",
    "    vocab_size=10000,\n",
    "    emb_size=10,\n",
    "    num_layers=2,\n",
    "    att_head_size=7,\n",
    "    num_heads=2,\n",
    "    att_out_size=15,\n",
    "    ff_hidden_size=20,\n",
    "    dropout_p=0.1,\n",
    ")\n",
    "\n",
    "tmp_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049bdc1d-c8f8-4483-91e0-3e11033e392c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_input = torch.randint(10000, (2, 5))\n",
    "\n",
    "print(f'Input shape: {tmp_input.shape}')\n",
    "tmp_output = tmp_layer(tmp_input)\n",
    "print(f'Output shape: {tmp_output.shape}')\n",
    "\n",
    "del tmp_input, tmp_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0566312-c9a0-4ce7-bb7c-0de7a8dc490f",
   "metadata": {},
   "source": [
    "### 1.4 Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569106a5-9886-4b9d-8ee8-09322e91c810",
   "metadata": {},
   "source": [
    "#### Картинка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92c1c76-bd4e-4fd7-9b69-a34979e2f939",
   "metadata": {},
   "source": [
    "<img src=\"https://i.pinimg.com/736x/c1/83/1a/c1831a58ecc935fc0f2ef4d35ce4fddb.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43308427-25ec-4d73-b632-84b6a814ac18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-20T11:40:51.214657Z",
     "iopub.status.busy": "2024-02-20T11:40:51.213938Z",
     "iopub.status.idle": "2024-02-20T11:40:51.236800Z",
     "shell.execute_reply": "2024-02-20T11:40:51.235622Z",
     "shell.execute_reply.started": "2024-02-20T11:40:51.214610Z"
    },
    "tags": []
   },
   "source": [
    "#### TransformerDecoderBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4692af8f-7ba7-40d1-a319-b2699f1e0d40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-20T11:40:51.214657Z",
     "iopub.status.busy": "2024-02-20T11:40:51.213938Z",
     "iopub.status.idle": "2024-02-20T11:40:51.236800Z",
     "shell.execute_reply": "2024-02-20T11:40:51.235622Z",
     "shell.execute_reply.started": "2024-02-20T11:40:51.214610Z"
    },
    "tags": []
   },
   "source": [
    "**Инициализация:**\n",
    "* _in_size_ ~ размер эмбеддингов на входе\n",
    "* _head_size_ ~ размер эмбеддингов матриц Q, K, V после преобразования\n",
    "* _num_heads_ ~ число голов attention\n",
    "* _out_size_ ~ размер эмбеддингов на выходе attention и блока\n",
    "* _ff_hidden_size_ ~ размер скрытого представления для линейных слоёв\n",
    "* _dropout_p_ ~ вероятность для dropout-ов\n",
    "* _encoder_out_size_ ~ размер эмбеддингов на выходе энкодера (если None, то in_size)\n",
    "\n",
    "**Forward:**\n",
    "* _decoder_emb_ ~ тензор, пришедший из предыдущего блока, или эмбеддинги с позиционными\n",
    "* _encoder_output_ ~ выходной тензор из соответствующего энкодера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f155f6-b3f5-41d5-84fb-a826069c0e7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Class with one full block within transformer's decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, in_size, head_size, num_heads, out_size, ff_hidden_size, dropout_p=0.2, encoder_out_size=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "           in_size: input embedding size\n",
    "           head_size: size of each attention head\n",
    "           num_heads: number of attention heads\n",
    "           out_size: output embedding size\n",
    "           ff_hidden_size: hidden size for feed forward net\n",
    "           dropout_p: probability for dropout\n",
    "           encoder_out_size: embedding size of outputs from encoder (if not provided - same as in_size)\n",
    "        \"\"\"\n",
    "        super(TransformerDecoderBlock, self).__init__()\n",
    "        \n",
    "        # Запишем все переданые гиперпараметры слоя\n",
    "        self.in_size = in_size\n",
    "        self.head_size = head_size\n",
    "        self.num_heads = num_heads\n",
    "        self.out_size = out_size\n",
    "        self.ff_hidden_size = ff_hidden_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.encoder_out_size = in_size if encoder_out_size is None else encoder_out_size\n",
    "        \n",
    "        \n",
    "        self.masked_attention = ...\n",
    "        # Если выход и вход attention-а имеют разный размер, то используем линейный слой на residual connection-е\n",
    "        self.adapt_residual = ...\n",
    "        self.norm = ...\n",
    "        self.dropout = ...\n",
    "        self.encoder_block = ...\n",
    "        \n",
    "    \n",
    "    def forward(self, decoder_emb, encoder_output):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "           decoder_emb: decoder sequence after embed\n",
    "           encoder_output: output from encoder\n",
    "        \"\"\"\n",
    "        # Получаем на вход тензор batch_size x seq_len x in_size и тензор batch_size x encoder_seq_len x encoder_out_size\n",
    "        mask = make_decoder_mask(decoder_emb)  # batch_size x 1 x seq_len x seq_len\n",
    "        attention = ...\n",
    "        residual_out = ...\n",
    "        norm_out = ...\n",
    "        block_out = ...\n",
    "        \n",
    "        return block_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aab54d-11d2-4619-a97b-cb26c76291e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Протестируем TransformerDecoderBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8e1323-d608-4bf2-91f3-549bebfeacc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_layer = TransformerDecoderBlock(\n",
    "    in_size=10,\n",
    "    head_size=7,\n",
    "    num_heads=2,\n",
    "    out_size=15,\n",
    "    ff_hidden_size=20,\n",
    "    dropout_p=0.1,\n",
    "    encoder_out_size=12,\n",
    ")\n",
    "\n",
    "tmp_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a9e1b5-ba17-4dfe-ae24-6b4aced2d8a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Проверяем в прямом проходе из декодера, где мы смешиваем информацию из энкодера и декодера\n",
    "tmp_input_decoder = torch.rand(2, 5, 10)\n",
    "tmp_output_encoder = torch.rand(2, 7, 12)\n",
    "\n",
    "print(\"Encoder+Decoder-like input\")\n",
    "print(f'Decoder input shape: {tmp_input_decoder.shape}')\n",
    "print(f'Encoder output shape: {tmp_output_encoder.shape}')\n",
    "\n",
    "tmp_output = tmp_layer(tmp_input_decoder, tmp_output_encoder)\n",
    "print(f'Output shape: {tmp_output.shape}')\n",
    "\n",
    "del tmp_input_decoder, tmp_output_encoder, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f13686-2123-468a-af3d-e150e798dbef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-20T11:40:51.214657Z",
     "iopub.status.busy": "2024-02-20T11:40:51.213938Z",
     "iopub.status.idle": "2024-02-20T11:40:51.236800Z",
     "shell.execute_reply": "2024-02-20T11:40:51.235622Z",
     "shell.execute_reply.started": "2024-02-20T11:40:51.214610Z"
    },
    "tags": []
   },
   "source": [
    "#### TransformerDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9219bd1-e968-4a6d-86e4-684803192472",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-20T11:40:51.214657Z",
     "iopub.status.busy": "2024-02-20T11:40:51.213938Z",
     "iopub.status.idle": "2024-02-20T11:40:51.236800Z",
     "shell.execute_reply": "2024-02-20T11:40:51.235622Z",
     "shell.execute_reply.started": "2024-02-20T11:40:51.214610Z"
    },
    "tags": []
   },
   "source": [
    "**Инициализация:**\n",
    "* _max_seq_len_ ~ максимальный размер в токенах последовательности\n",
    "* _vocab_size_ ~ размер словаря\n",
    "* _emb_size_ ~ размер эмбеддингов на входе\n",
    "* _num_layers_ ~ число TransformerEncoderBlock-ов\n",
    "* _att_out_size_ ~ размер эмбеддингов на выходе attention и блока\n",
    "* _att_head_size_ ~ размер эмбеддингов матриц Q, K, V после преобразования\n",
    "* _num_heads_ ~ число голов attention\n",
    "* _ff_hidden_size_ ~ размер скрытого представления для линейных слоёв\n",
    "* _dropout_p_ ~ вероятность для dropout-ов\n",
    "* _encoder_out_size_ ~ размер эмбеддингов на выходе энкодера (если None, то in_size)\n",
    "\n",
    "**Forward:**\n",
    "* _decoder_input_ ~ токены входа в декодер до эмбеддингов\n",
    "* _encoder_output_ ~ выходной тензор из соответствующего энкодера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0b2859-d668-4696-850f-aca7b73ead7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Class for decoder within transformer.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_seq_len, vocab_size, emb_size, num_layers, att_out_size, att_head_size, num_heads, ff_hidden_size, dropout_p, encoder_out_size=None):\n",
    "        \"\"\"  \n",
    "        Args:\n",
    "            max_seq_len : maximum length of input sequence\n",
    "            vocab_size: size of the vocabulary\n",
    "            emb_size: embeddings size\n",
    "            num_layers: number of encoder layers\n",
    "            att_out_size: output size for attention and each encoder block\n",
    "            att_head_size: size of each attention head\n",
    "            num_heads: number of heads in multihead attention\n",
    "            ff_hidden_size: hidden size for feed forward net\n",
    "            dropout_p: probability for dropout\n",
    "            encoder_out_size: embedding size of outputs from encoder (if not provided - same as in_size)\n",
    "        \"\"\"\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "                \n",
    "        # Запишем все переданые гиперпараметры слоя\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.num_layers = num_layers\n",
    "        self.att_out_size = att_out_size\n",
    "        self.att_head_size = att_head_size\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_hidden_size = ff_hidden_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.encoder_out_size = in_size if encoder_out_size is None else encoder_out_size\n",
    "        \n",
    "        self.embedding_layer = nn.Embedding(self.vocab_size, self.emb_size)\n",
    "        self.positional_encoder = ...\n",
    "        self.dropout = ...\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleDict({\n",
    "            f\"decoder_block_{i}\": ... for i in range(self.num_layers)\n",
    "        })\n",
    "        \n",
    "        self.fc = ...\n",
    "\n",
    "    def forward(self, decoder_input, encoder_output):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            decoder_input:\n",
    "            encoder_output:\n",
    "        Returns:\n",
    "            out: output vector\n",
    "        \"\"\"\n",
    "        # Получаем на вход batch_size x seq_len и batch_size x encoder_seq_len x encoder_out_size\n",
    "        decoder_emb = self.embedding_layer(decoder_input)  # batch_size x seq_len x emb_size\n",
    "        decoder_emb_pos = ...\n",
    "        \n",
    "        out = ...\n",
    "     \n",
    "        for block in self.decoder_blocks.values():\n",
    "            out = ...\n",
    "\n",
    "        logits = ...\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0536e4c0-fb20-458b-8a45-b7d6ba0314a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Протестируем TransformerDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b1ff27-7320-473c-a72b-8442eff5f423",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_layer = TransformerDecoder(\n",
    "    max_seq_len=20,\n",
    "    vocab_size=10000,\n",
    "    emb_size=10,\n",
    "    num_layers=2,\n",
    "    att_head_size=7,\n",
    "    num_heads=2,\n",
    "    att_out_size=15,\n",
    "    ff_hidden_size=20,\n",
    "    dropout_p=0.1,\n",
    "    encoder_out_size=12,\n",
    ")\n",
    "\n",
    "tmp_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d819cf-374e-4ef4-9153-d4b2e3252092",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Проверяем в прямом проходе из декодера, где мы смешиваем информацию из энкодера и декодера\n",
    "tmp_input_decoder = torch.randint(10000, (2, 5))\n",
    "tmp_output_encoder = torch.rand(2, 7, 12)\n",
    "\n",
    "print(\"Encoder+Decoder-like input\")\n",
    "print(f'Decoder input shape: {tmp_input_decoder.shape}')\n",
    "print(f'Encoder output shape: {tmp_output_encoder.shape}')\n",
    "\n",
    "tmp_output = tmp_layer(tmp_input_decoder, tmp_output_encoder)\n",
    "print(f'Output shape: {tmp_output.shape}')\n",
    "\n",
    "del tmp_input_decoder, tmp_output_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6c6a6d-48d2-4033-a578-1ac1221f08b0",
   "metadata": {},
   "source": [
    "### 1.5 Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6147cef-1674-43ff-bade-be5ac114e00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Class for full encoder-deccoder transformer\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_seq_len,\n",
    "        vocab_size,\n",
    "        emb_size,\n",
    "        \n",
    "        num_encoder_layers,\n",
    "        enc_att_out_size,\n",
    "        enc_att_head_size,\n",
    "        enc_num_heads,\n",
    "        enc_ff_hidden_size,\n",
    "        enc_dropout_p,\n",
    "        \n",
    "        num_decoder_layers,\n",
    "        dec_att_out_size,\n",
    "        dec_att_head_size,\n",
    "        dec_num_heads,\n",
    "        dec_ff_hidden_size,\n",
    "        dec_dropout_p,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        # Запишем все переданые гиперпараметры модели\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        \n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.enc_att_out_size = enc_att_out_size\n",
    "        self.enc_att_head_size = enc_att_head_size\n",
    "        self.enc_num_heads = enc_num_heads\n",
    "        self.enc_ff_hidden_size = enc_ff_hidden_size\n",
    "        self.enc_dropout_p = enc_dropout_p\n",
    "        \n",
    "        self.num_decoder_layers = num_decoder_layers\n",
    "        self.dec_att_out_size = dec_att_out_size\n",
    "        self.dec_att_head_size = dec_att_out_size\n",
    "        self.dec_num_heads = dec_num_heads\n",
    "        self.dec_ff_hidden_size = dec_ff_hidden_size\n",
    "        self.dec_dropout_p = dec_dropout_p\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = TransformerEncoder(\n",
    "            max_seq_len=self.max_seq_len,\n",
    "            vocab_size=self.vocab_size,\n",
    "            emb_size=self.emb_size,\n",
    "            num_layers=self.num_encoder_layers,\n",
    "            att_head_size=self.enc_att_head_size,\n",
    "            num_heads=self.enc_num_heads,\n",
    "            att_out_size=self.enc_att_out_size,\n",
    "            ff_hidden_size=self.enc_ff_hidden_size,\n",
    "            dropout_p=self.enc_dropout_p,\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = TransformerDecoder(\n",
    "            max_seq_len=self.max_seq_len,\n",
    "            vocab_size=self.vocab_size,\n",
    "            emb_size=self.emb_size,\n",
    "            num_layers=self.num_decoder_layers,\n",
    "            att_head_size=self.dec_att_head_size,\n",
    "            num_heads=self.dec_num_heads,\n",
    "            att_out_size=self.dec_att_out_size,\n",
    "            ff_hidden_size=self.dec_ff_hidden_size,\n",
    "            dropout_p=self.dec_dropout_p,\n",
    "            encoder_out_size=self.enc_att_out_size,\n",
    "        )\n",
    "    \n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encoder_input: input to encoder \n",
    "            decoder_input: input to decoder\n",
    "        out:\n",
    "            out: final tensor with logits of each word in vocab\n",
    "        \"\"\"\n",
    "        # Получаем на вход batch_size x enc_seq_len и batch_size x dec_seq_len\n",
    "        encoder_output = self.encoder(encoder_input)  # (batch_size, enc_seq_len, enc_att_out_size)\n",
    "   \n",
    "        return self.decoder(decoder_input, encoder_output)  # (batch_size, dec_seq_len, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dadcfc-be2d-4715-9a46-3917865c1619",
   "metadata": {},
   "source": [
    "### 1.6 Тестируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad2cdcb-8b28-4b90-a161-04fc0e9e97e9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_layer = Transformer(\n",
    "    max_seq_len=20,\n",
    "    vocab_size=10000,\n",
    "    emb_size=10,\n",
    "    \n",
    "    num_encoder_layers=3,\n",
    "    enc_att_head_size=7,\n",
    "    enc_num_heads=3,\n",
    "    enc_att_out_size=20,\n",
    "    enc_ff_hidden_size=30,\n",
    "    enc_dropout_p=0.2,\n",
    "    \n",
    "    num_decoder_layers=2,\n",
    "    dec_att_head_size=7,\n",
    "    dec_num_heads=2,\n",
    "    dec_att_out_size=15,\n",
    "    dec_ff_hidden_size=20,\n",
    "    dec_dropout_p=0.1,\n",
    ")\n",
    "\n",
    "tmp_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21adac3b-fe8e-43f3-8841-a32a4db62dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_input_encoder = torch.randint(10000, (2, 9))\n",
    "tmp_input_decoder = torch.randint(10000, (2, 5))\n",
    "\n",
    "print(f'Encoder input shape: {tmp_input_encoder.shape}')\n",
    "print(f'Decoder input shape: {tmp_input_decoder.shape}')\n",
    "\n",
    "tmp_output = tmp_layer(tmp_input_encoder, tmp_input_decoder)\n",
    "print(f'Output shape: {tmp_output.shape}')\n",
    "\n",
    "del tmp_input_decoder, tmp_input_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b16d861-7257-43ab-93b8-abf6e05812b5",
   "metadata": {},
   "source": [
    "### 1.7 Дополнительно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ff08e2-84a4-4c69-9c49-8378099ea103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "?torch.nn.Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ef7e89-4a88-4640-b636-35c1153f0062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "notebookId": "00017dc0-69ed-43c3-b989-8029237a77d0",
  "notebookPath": "Sem5 - Interpretation/sem5_interpretation.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

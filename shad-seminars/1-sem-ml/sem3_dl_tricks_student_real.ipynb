{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "yseqwocngnoxu8yc4ctuw"
   },
   "source": [
    "# **Seminar 3 - Deep Learning tricks**\n",
    "*Naumov Anton (Any0019)*\n",
    "\n",
    "*To contact me in telegram: @any0019*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "60j9x1hegnt7b4u1lu8fyd"
   },
   "source": [
    "## 1. Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "n2t71o3buofm3hbssbyut"
   },
   "source": [
    "### 1.1 Скачаем и подготовим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "rsz5coml546a5l89bz5xr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data = {}\n",
    "\n",
    "# CIFAR10 ~ smaller, use this one if you want faster performance\n",
    "# dataset_name = \"CIFAR10\"\n",
    "# data['train'] = datasets.CIFAR10(root='./data', download=True, train=True)\n",
    "# data['test'] = datasets.CIFAR10(root='./data', download=True, train=False)\n",
    "\n",
    "# STL10 ~ bigger, use this one if you want more representitive results\n",
    "dataset_name = \"STL10\"\n",
    "data['train'] = datasets.STL10(root='./data', download=True, split='train')\n",
    "data['test'] = datasets.STL10(root='./data', download=True, split='test')\n",
    "\n",
    "\n",
    "clear_output(True)\n",
    "train_size = len(data['train'])\n",
    "test_size = len(data['test'])\n",
    "img = data['train'][0][0]\n",
    "img_size = [len(img.getbands()), *img.size]\n",
    "if len(img_size) == 2:\n",
    "    img_size = [1, *img_size]\n",
    "classes = data['train'].classes\n",
    "classes_ = '\\n  --'.join(classes)\n",
    "print(\n",
    "    f'sizes:\\n  train ~ {train_size}\\n  test ~ {test_size}\\n' +\n",
    "    f'\\noutput:\\n  images of shape ~ {img_size}\\n\\n  classes:\\n  --{classes_}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "rsz5coml546a5l89bz5xr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "h = 2\n",
    "w = 5\n",
    "\n",
    "fig, ax = plt.subplots(h, w, figsize=(20, 5 * h))\n",
    "fig.suptitle(f'{dataset_name}:', y=0.9)\n",
    "for i, el in enumerate(data[\"train\"]):\n",
    "    if i >= h * w:\n",
    "        break\n",
    "    plt.subplot(h, w, i + 1)\n",
    "    plt.imshow(el[0])\n",
    "    plt.title(data[\"train\"].classes[el[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "avistm9aynh7usx0ylrtqg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "# CODE ... IN THIS BLOCK\n",
    "# Посчитайте поканальные среднее и стандартное отклонение для всех изображений из train выборки\n",
    "\n",
    "channel_mean = np.zeros((img_size[0]))\n",
    "channel_std = np.zeros((img_size[0]))\n",
    "\n",
    "for img, _ in tqdm(data['train']):\n",
    "    ...\n",
    "\n",
    "channel_mean /= len(data['train'])\n",
    "channel_std /= len(data['train'])\n",
    "\n",
    "print(channel_mean, channel_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "3akkmage08ctsh9iyg50z",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, Subset\n",
    "import torchvision.transforms as tr\n",
    "\n",
    "class AddTransformsDataset(Dataset):\n",
    "    # Class to add custom transforms to any dataset\n",
    "    def __init__(self, dataset, transforms=None, indices=None):\n",
    "        if indices is not None:\n",
    "            self.dataset = Subset(dataset, indices)\n",
    "        else:\n",
    "            self.dataset = dataset\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.dataset.__len__()\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        image, label = self.dataset.__getitem__(index)\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_dataset = AddTransformsDataset(data[\"train\"], None)\n",
    "img, label = tmp_dataset[0]\n",
    "print(label, classes[label])\n",
    "img\n",
    "\n",
    "del tmp_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Иллюстрации аугументаций из библиотеки torchvision.transforms](https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_illustrations.html#sphx-glr-auto-examples-transforms-plot-transforms-illustrations-py)\n",
    "\n",
    "[Общая дока по torchvision.transforms](https://pytorch.org/vision/stable/transforms.html)\n",
    "\n",
    "[Albumentations](https://albumentations.ai/) и их же [Github](https://github.com/albumentations-team/albumentations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "ef6wcta4xy82m4azldegn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_transform():\n",
    "    return tr.Compose([\n",
    "        tr.RandomHorizontalFlip(0.5),\n",
    "        tr.ToTensor(),\n",
    "        tr.Normalize(channel_mean, channel_std),\n",
    "    ])\n",
    "\n",
    "def val_transform():\n",
    "    return tr.Compose([\n",
    "        tr.ToTensor(),\n",
    "        tr.Normalize(channel_mean, channel_std),\n",
    "    ])\n",
    "\n",
    "\n",
    "def make_dataloaders(\n",
    "    data,\n",
    "    batch_size=16,\n",
    "    train_transform_function=None,\n",
    "    val_transform_function=None,\n",
    "    val_part=0.5,\n",
    "    num_workers=0,\n",
    "):\n",
    "    dataloaders = dict()\n",
    "    \n",
    "    ds_train = AddTransformsDataset(data['train'], train_transform_function())\n",
    "    dataloaders['train'] = DataLoader(\n",
    "        ds_train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    \n",
    "    perm = torch.randperm(len(data['test']))\n",
    "    val_size = round(len(data[\"test\"]) * val_part)\n",
    "    \n",
    "    \n",
    "    ds_val = AddTransformsDataset(data['test'], val_transform_function(), perm[:val_size])\n",
    "    dataloaders['val'] = DataLoader(\n",
    "        ds_val,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    \n",
    "    ds_test = AddTransformsDataset(data['test'], val_transform_function(), perm[val_size:])\n",
    "    dataloaders['test'] = DataLoader(\n",
    "        ds_test,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    \n",
    "    return dataloaders\n",
    "\n",
    "\n",
    "dataloaders = make_dataloaders(\n",
    "    data,\n",
    "    batch_size=64, # 64 works well for CIFAR10\n",
    "    train_transform_function=train_transform,\n",
    "    val_transform_function=val_transform,\n",
    "    val_part=0.5,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "print('sizes:')\n",
    "for split in dataloaders:\n",
    "    print(f'  {split} ~ {len(dataloaders[split].dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "fnul2f4bozubfdypmsg7ff",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(30, 20))\n",
    "for i, split in enumerate(dataloaders):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.title(split)\n",
    "    size = len(dataloaders[split].dataset)\n",
    "    cnt = Counter([dataloaders[split].dataset[i][1]\n",
    "                   for i in range(size)])\n",
    "    wedges, texts, autotexts = plt.pie(\n",
    "        [cnt[j] for j in range(len(classes))],\n",
    "        autopct=lambda pct: '{:.1f}% ({:d})'.format(pct, int(pct/100.*size)),\n",
    "        textprops=dict(color=\"black\"),\n",
    "        colors=plt.cm.Dark2.colors,\n",
    "        startangle=0,\n",
    "        explode=[0.1]*len(classes),\n",
    "    )\n",
    "    plt.legend(wedges, classes, title='Image classes', loc=\"center left\", bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "    plt.setp(autotexts, size=10, weight=700)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "q905dh9hfaltbmdh37mq4b"
   },
   "source": [
    "### 1.2 Базовая модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "pycywhr7zmilmfl35j5go",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "# CODE ... IN THIS BLOCK\n",
    "\n",
    "class residual_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c, kernel_size, activation, dropout_p, pool, i):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(OrderedDict([\n",
    "            (f'conv', ...),\n",
    "            (f'bnorm', ...),\n",
    "            (f'drop', ...),\n",
    "            (f'act', ...),\n",
    "        ]))\n",
    "        self.pool = pool\n",
    "        self.i = i\n",
    "        if in_c != out_c and self.i:\n",
    "            self.conv1 = ...\n",
    "        else:\n",
    "            self.conv1 = ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.i:\n",
    "            return ...  # Residual connection\n",
    "        else:\n",
    "            return ...\n",
    "\n",
    "\n",
    "class conv_net(nn.Module):\n",
    "    \"\"\"\n",
    "        Simple convolutional network\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        num_conv_layers,\n",
    "        kernel_sizes,\n",
    "        hidden_size,\n",
    "        activations,\n",
    "        dropouts,\n",
    "        poolings,\n",
    "        output_size,\n",
    "        use_residual=True,\n",
    "    ):\n",
    "        super(conv_net, self).__init__()\n",
    "        \n",
    "        if not isinstance(kernel_sizes, list):\n",
    "            kernel_sizes = [kernel_sizes] * num_conv_layers\n",
    "        assert len(kernel_sizes) == num_conv_layers, f'provide {num_conv_layers} kernel_sizes or just one for all layers'\n",
    "        \n",
    "        assert isinstance(hidden_size, int), f'provide one hidden_size for all layers'\n",
    "        hidden_sizes = [input_size[0]] + [hidden_size] * num_conv_layers\n",
    "        \n",
    "        if not isinstance(activations, list):\n",
    "            activations = [activations] * num_conv_layers\n",
    "        assert len(activations) == num_conv_layers, f'provide {num_conv_layers} activation functions or just one for all layers'\n",
    "        \n",
    "        if not isinstance(dropouts, list):\n",
    "            dropouts = [dropouts] * num_conv_layers\n",
    "        assert len(dropouts) == num_conv_layers, f'provide {num_conv_layers} dropout values or just one for all layers'\n",
    "        \n",
    "        if not isinstance(poolings, list):\n",
    "            poolings = [poolings] * num_conv_layers\n",
    "        assert len(poolings) == num_conv_layers, f'provide {num_conv_layers} poolings or just one for all layers'\n",
    "        available_poolings = {None, nn.MaxPool2d, nn.AvgPool2d, nn.Identity}\n",
    "        assert np.sum([poolings[i] in available_poolings for i in range(num_conv_layers)]) == num_conv_layers, \\\n",
    "               f'each pooling should be one of {available_poolings}'\n",
    "    \n",
    "        \n",
    "        self.poolings = [poolings[i](2) if poolings[i] else nn.Identity() for i in range(num_conv_layers)]\n",
    "        \n",
    "        if use_residual:\n",
    "            self.conv_blocks = nn.Sequential(*[residual_block(\n",
    "                in_c=...,\n",
    "                out_c=...,\n",
    "                kernel_size=...,\n",
    "                activation=...,\n",
    "                dropout_p=...,\n",
    "                pool=...,\n",
    "                i=i,\n",
    "            ) for i in range(num_conv_layers)])\n",
    "        else:\n",
    "            conv_blocks = [[\n",
    "                (f'conv_{i+1}', ...),\n",
    "                (f'bnorm_{i+1}', ...),\n",
    "                (f'drop_{i+1}', ...),\n",
    "                (f'act_{i+1}', ...),\n",
    "                (f'pool_{i+1}', ...),\n",
    "            ] for i in range(num_layers)]\n",
    "            self.conv_blocks = []\n",
    "            for block in conv_blocks:\n",
    "                self.conv_blocks.extend(block)\n",
    "            self.conv_blocks = nn.Sequential(OrderedDict(self.conv_blocks))\n",
    "        \n",
    "        head = [\n",
    "            ('hid2out', ...),\n",
    "            ('log_softmax', ...)\n",
    "        ]\n",
    "        self.head = nn.Sequential(OrderedDict(head))\n",
    "    \n",
    "    def forward(self, imgs):\n",
    "        out3d = self.conv_blocks(imgs)\n",
    "        out1d = torch.amax(out3d, dim=(2, 3))  # Global max pooling\n",
    "        return self.head(out1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "upjnxc8ldm5tvumjrdw0t",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = conv_net(\n",
    "    input_size = img_size,\n",
    "    num_conv_layers = 5,\n",
    "    kernel_sizes = 5,\n",
    "    hidden_size = 50,\n",
    "    activations = nn.LeakyReLU(0.2),\n",
    "    dropouts = 0.2,\n",
    "    poolings = nn.AvgPool2d,\n",
    "    output_size = len(classes)\n",
    ")\n",
    "\n",
    "print('Model:', model, sep='\\n')\n",
    "\n",
    "imgs_batch, lbls_batch = next(iter(dataloaders['train']))\n",
    "\n",
    "print(f'\\nInput shape: {imgs_batch.shape}')\n",
    "out = model(imgs_batch)\n",
    "print(f'Output shape: {out.shape}')\n",
    "\n",
    "prob_sums = out.exp().sum(-1).detach().numpy()\n",
    "assert np.allclose(prob_sums, 1), 'all exp-sums must be close to 1, returned not log-probabilities'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "suvayf981lht1hotq9y9c"
   },
   "outputs": [],
   "source": [
    "#!g1.2\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "blg6e48jg14iwopfaicp7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Counting how many parameters does our model have\n",
    "def pretty_num(num):\n",
    "    num = str(num)\n",
    "    num = reversed([num[max(0, i-2):i+1] for i in range(len(num)-1, -1, -3)])\n",
    "    return \".\".join(num)\n",
    "\n",
    "def model_num_params(model, print_=True):\n",
    "    sum_params = 0\n",
    "    for param in model.named_parameters():\n",
    "        num_params = param[1].numel()\n",
    "        if print_:\n",
    "            print('{: <35} ~  {: <7} params'.format(param[0], pretty_num(num_params)))\n",
    "        sum_params += num_params\n",
    "    print(f'\\nIn total: {pretty_num(sum_params)} params')\n",
    "    return sum_params\n",
    "\n",
    "sum_params = model_num_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "gvie7ohyh3cmzf9gwbwye"
   },
   "source": [
    "### 1.3 Оптимайзер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "vffx3yinnsn16dmxww1hq9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def create_model_and_optimizer(model_class, model_params, lr=1e-3, beta1=0.9, beta2=0.999, device=device):\n",
    "    model = model_class(**model_params)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    params = []\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            params.append(param)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(params, lr, [beta1, beta2])\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "24lmuanimgyswocm79cqxh"
   },
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'input_size': img_size,\n",
    "    'num_conv_layers': 5,\n",
    "    'kernel_sizes': 5,\n",
    "    'hidden_size': 50,\n",
    "    'activations': nn.LeakyReLU(0.2),\n",
    "    'dropouts': 0.2,\n",
    "    'poolings': nn.MaxPool2d,\n",
    "    'output_size': len(classes),\n",
    "}\n",
    "\n",
    "model, optimizer = create_model_and_optimizer(\n",
    "    model_class = conv_net, \n",
    "    model_params = model_params,\n",
    "    lr = 1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "zf3o0z9pqvle2z3pljfcni"
   },
   "source": [
    "### 1.4 Обучение/валидация на одном шаге"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "gfxhus51xwszw8hc9b8ir"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# CODE ... IN THIS BLOCK\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "def train(model, optimizer, loader, criterion):\n",
    "    model.train()\n",
    "    losses_tr = []\n",
    "    for images, targets in tqdm(loader):\n",
    "        # Зануляем градиенты\n",
    "        ...\n",
    "        \n",
    "        # Перенести всё на нужный device\n",
    "        ...\n",
    "        \n",
    "        # Прямой проход и подсчёт ошибки\n",
    "        ...\n",
    "        loss = ...\n",
    "        \n",
    "        # Обратный проход, шаг оптимайзера\n",
    "        ...\n",
    "        \n",
    "        losses_tr.append(loss.item()) \n",
    "    \n",
    "    return model, optimizer, np.mean(losses_tr)\n",
    "\n",
    "def val(model, loader, criterion, metric_names=None):\n",
    "    model.eval()\n",
    "    losses_val = []\n",
    "    if metric_names:\n",
    "        metrics = {name: [] for name in metric_names}\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(loader):\n",
    "            # Аналогично обучению, но без обратного прохода и шага оптимайзера\n",
    "            ...\n",
    "            loss = ...\n",
    "            losses_val.append(loss.item())\n",
    "            \n",
    "            if metric_names:\n",
    "                if 'accuracy' in metrics:\n",
    "                    _, pred_classes = torch.max(out, dim=-1)\n",
    "                    metrics['accuracy'].append((pred_classes == targets).float().mean().item())\n",
    "    \n",
    "        if metric_names:\n",
    "            for name in metrics:\n",
    "                metrics[name] = np.mean(metrics[name])\n",
    "    \n",
    "    return np.mean(losses_val), metrics if metric_names else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "hbtmxirv4i7zvozi5yitw"
   },
   "source": [
    "### 1.5 Цикл обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "rbnknybd7ecizj8p6o445b"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "    \n",
    "\n",
    "def learning_loop(model, optimizer, train_loader, val_loader, criterion, scheduler=None, min_lr=None, epochs=10, val_every=1, draw_every=1, metric_names=None):\n",
    "    losses = {'train': [], 'val': []}\n",
    "    if metric_names:\n",
    "        metrics = {name: [] for name in metric_names}\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        print(f'#{epoch}/{epochs}:')\n",
    "        model, optimizer, loss = train(model, optimizer, train_loader, criterion)\n",
    "        losses['train'].append(loss)\n",
    "\n",
    "        if not (epoch % val_every):\n",
    "            loss, metrics_ = val(model, val_loader, criterion, metric_names)\n",
    "            losses['val'].append(loss)\n",
    "            if metric_names:\n",
    "                for name in metrics_:\n",
    "                    metrics[name].append(metrics_[name])\n",
    "            if scheduler:\n",
    "                try:\n",
    "                    scheduler.step()\n",
    "                except:\n",
    "                    scheduler.step(loss)\n",
    "\n",
    "        if not (epoch % draw_every):\n",
    "            clear_output(True)\n",
    "            ww = 2 if metric_names else 1\n",
    "            fig, ax = plt.subplots(1, ww, figsize=(20, 10))\n",
    "            fig.suptitle(f'#{epoch}/{epochs}:')\n",
    "\n",
    "            plt.subplot(1, ww, 1)\n",
    "            plt.title('losses')\n",
    "            plt.plot(losses['train'], 'r.-', label='train')\n",
    "            plt.plot(losses['val'], 'g.-', label='val')\n",
    "            plt.legend()\n",
    "            \n",
    "            if metric_names:\n",
    "                plt.subplot(1, ww, 2)\n",
    "                plt.title('additional metrics')\n",
    "                for name in metric_names:\n",
    "                    plt.plot(metrics[name], '.-', label=name)\n",
    "                plt.legend()\n",
    "            \n",
    "            plt.show()\n",
    "        \n",
    "        if min_lr and get_lr(optimizer) <= min_lr:\n",
    "            print(f'Learning process ended with early stop after epoch {epoch}')\n",
    "            break\n",
    "    \n",
    "    return model, optimizer, losses, metrics if metric_names else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "ow5gsaejnfn787qco7val"
   },
   "source": [
    "### 1.6 Обучаем базовую модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "37nygqlxsj3ykpbiilnsf"
   },
   "outputs": [],
   "source": [
    "models_params = dict()\n",
    "models = dict()\n",
    "optimizers = dict()\n",
    "schedulers = dict()\n",
    "criterions = dict()\n",
    "losses = dict()\n",
    "metrics = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "qvu23bh9ddosb9v0lly9z"
   },
   "outputs": [],
   "source": [
    "model_type = 'basic'\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model_params = {\n",
    "    'input_size': img_size,\n",
    "    'num_conv_layers': 5,\n",
    "    'kernel_sizes': 5,\n",
    "    'hidden_size': 50,\n",
    "    'activations': nn.ReLU(),\n",
    "    'dropouts': 0.2,\n",
    "    'poolings': nn.MaxPool2d,\n",
    "    'output_size': len(classes),\n",
    "    'use_residual': True,\n",
    "}\n",
    "\n",
    "models[model_type], optimizers[model_type] = create_model_and_optimizer(\n",
    "    model_class = conv_net, \n",
    "    model_params = model_params,\n",
    "    lr = 1e-3,\n",
    "    device = device,\n",
    ")\n",
    "\n",
    "schedulers[model_type] = None\n",
    "\n",
    "criterions[model_type] = nn.NLLLoss()\n",
    "\n",
    "print(models[model_type])\n",
    "_ = model_num_params(models[model_type], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "cei7286u8lea97faehkb2w"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "models[model_type], optimizers[model_type], losses[model_type], metrics[model_type] = learning_loop(\n",
    "    model = models[model_type],\n",
    "    optimizer = optimizers[model_type],\n",
    "    train_loader = dataloaders['train'],\n",
    "    val_loader = dataloaders['val'],\n",
    "    criterion = criterions[model_type],\n",
    "    scheduler = schedulers[model_type],\n",
    "    epochs = 20,\n",
    "    min_lr = None,\n",
    "    metric_names = {'accuracy'},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "3z6epn4ct122espga17pyo"
   },
   "source": [
    "## 2. Интересные методы над уже обученными моделями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "ehz5pf30jcm9fl28vvlulf"
   },
   "source": [
    "### 2.1 Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "iwwo8msnm5bjmx0lsj2l69"
   },
   "source": [
    "Ситуация: поставлена задача, но данных непосредственно для этой задачи мало, при этом существуют датасеты близкие к данному\n",
    "\n",
    "Идея: возьмём нейросеть полноценно обученную на схожую задачу и ___дообучим___ модель на наших данных\n",
    "\n",
    "Предобученные модели для классификации изображений в PyTorch: https://pytorch.org/vision/stable/models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "vogp3pjb0zespvsrj3bwd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import models as mds\n",
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "model_type = 'resnet_finetune'\n",
    "\n",
    "resnet_weights = mds.ResNet18_Weights.DEFAULT\n",
    "models[model_type] = mds.resnet18(weights=resnet_weights, progress=True)\n",
    "\n",
    "print(models[model_type])\n",
    "_ = model_num_params(models[model_type], False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "po579sjsyomui5ocqhy5h8"
   },
   "source": [
    "По сути мы просто обучаемся не со случайного стартового состояния, а с уже обученного состояния\n",
    "\n",
    "**ОЧЕНЬ ВАЖНО:** Используйте меньший learning rate, чтобы первыми несколькими шагами мы и не сломали бы всю предобработку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "06dezvbvk0dhu71rvozmrag",
    "tags": []
   },
   "outputs": [],
   "source": [
    "models[model_type].fc = nn.Linear(models[model_type].fc.in_features, len(classes))\n",
    "\n",
    "models[model_type] = models[model_type].to(device)\n",
    "\n",
    "params = []\n",
    "for param in models[model_type].parameters():\n",
    "    if param.requires_grad:\n",
    "        params.append(param)\n",
    "\n",
    "optimizers[model_type] = torch.optim.Adam(params, 1e-6, [0.9, 0.999])\n",
    "\n",
    "schedulers[model_type] = None\n",
    "\n",
    "criterions[model_type] = nn.CrossEntropyLoss()\n",
    "\n",
    "print(models[model_type])\n",
    "_ = model_num_params(models[model_type], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "rh2splr2as0xc39b1ml6k"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "models[model_type], optimizers[model_type], losses[model_type], metrics[model_type] = learning_loop(\n",
    "    model = models[model_type],\n",
    "    optimizer = optimizers[model_type],\n",
    "    train_loader = dataloaders['train'],\n",
    "    val_loader = dataloaders['val'],\n",
    "    criterion = criterions[model_type],\n",
    "    scheduler = schedulers[model_type],\n",
    "    epochs = 20,\n",
    "    min_lr = None,\n",
    "    metric_names = {'accuracy'},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "jf8npskuqnpbz2ad30k134"
   },
   "source": [
    "### 2.2 Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "fo6egcywj0o6lqoztflqbb"
   },
   "source": [
    "Идея очень близкая к finetuning\n",
    "\n",
    "Главная идея и отличие в том, что здесь мы уже хотим использовать предобученную модель чисто как фича-экстрактор. Мы замораживаем всю модель, кроме головы, а голову заменяем на необходимую нам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "qk0baxjgmqpwk8imrdxt"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_type = 'resnet_transfer'\n",
    "\n",
    "models[model_type] = mds.resnet18(weights=resnet_weights, progress=True)\n",
    "\n",
    "for param in models[model_type].parameters():\n",
    "    param.requires_grad = False  # Freezing all layers\n",
    "\n",
    "models[model_type].fc = nn.Linear(models[model_type].fc.in_features, len(classes))\n",
    "\n",
    "models[model_type] = models[model_type].to(device)\n",
    "\n",
    "params = []\n",
    "for param in models[model_type].parameters():\n",
    "    if param.requires_grad:\n",
    "        params.append(param)\n",
    "\n",
    "optimizers[model_type] = torch.optim.Adam(params, 1e-3, [0.9, 0.999])\n",
    "\n",
    "schedulers[model_type] = None\n",
    "\n",
    "criterions[model_type] = nn.CrossEntropyLoss()\n",
    "\n",
    "models[model_type], optimizers[model_type], losses[model_type], metrics[model_type] = learning_loop(\n",
    "    model = models[model_type],\n",
    "    optimizer = optimizers[model_type],\n",
    "    train_loader = dataloaders['train'],\n",
    "    val_loader = dataloaders['val'],\n",
    "    criterion = criterions[model_type],\n",
    "    scheduler = schedulers[model_type],\n",
    "    epochs = 20,\n",
    "    min_lr = None,\n",
    "    metric_names = {'accuracy'},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "57cxktphzpceeghwebsc1c"
   },
   "source": [
    "## 3. Что можно сделать с lr?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "zeapezssmg6sgbz0prrzo"
   },
   "source": [
    "### 3.1 Learning rate schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "fmqc736n39650lsfcedkgc"
   },
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "    \n",
    "\n",
    "def learning_loop_with_lr(model, optimizer, train_loader, val_loader, criterion, scheduler=None, min_lr=None, epochs=10, val_every=1, draw_every=1, metric_names=None):\n",
    "    losses = {'train': [], 'val': []}\n",
    "    lrs = []\n",
    "    if metric_names:\n",
    "        metrics = {name: [] for name in metric_names}\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        print(f'#{epoch}/{epochs}:')\n",
    "        model, optimizer, loss = train(model, optimizer, train_loader, criterion)\n",
    "        losses['train'].append(loss)\n",
    "\n",
    "        if not (epoch % val_every):\n",
    "            loss, metrics_ = val(model, val_loader, criterion, metric_names)\n",
    "            losses['val'].append(loss)\n",
    "            if metric_names:\n",
    "                for name in metrics_:\n",
    "                    metrics[name].append(metrics_[name])\n",
    "            \n",
    "            lrs.append(get_lr(optimizer))\n",
    "            if scheduler:\n",
    "                try:\n",
    "                    scheduler.step()\n",
    "                except:\n",
    "                    scheduler.step(loss)\n",
    "\n",
    "        if not (epoch % draw_every):\n",
    "            clear_output(True)\n",
    "            ww = 3 if metric_names else 2\n",
    "            fig, ax = plt.subplots(1, ww, figsize=(20, 10))\n",
    "            fig.suptitle(f'#{epoch}/{epochs}:')\n",
    "\n",
    "            plt.subplot(1, ww, 1)\n",
    "            plt.title('losses')\n",
    "            plt.plot(losses['train'], 'r.-', label='train')\n",
    "            plt.plot(losses['val'], 'g.-', label='val')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.subplot(1, ww, 2)\n",
    "            plt.title('learning rate')\n",
    "            plt.plot(lrs, '.-', label='lr')\n",
    "            plt.legend()\n",
    "            \n",
    "            if metric_names:\n",
    "                plt.subplot(1, ww, 3)\n",
    "                plt.title('additional metrics')\n",
    "                for name in metric_names:\n",
    "                    plt.plot(metrics[name], '.-', label=name)\n",
    "                plt.legend()\n",
    "            \n",
    "            plt.show()\n",
    "        \n",
    "        if min_lr and get_lr(optimizer) <= min_lr:\n",
    "            print(f'Learning process ended with early stop after epoch {epoch}')\n",
    "            break\n",
    "    \n",
    "    return model, optimizer, losses, lrs, metrics if metric_names else None\n",
    "\n",
    "lrs = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "g67vk6tvlr3f0g4ifitxm"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_type = 'lr_exponential'\n",
    "\n",
    "models[model_type] = mds.resnet18(weights=resnet_weights, progress=True)\n",
    "\n",
    "for param in models[model_type].parameters():\n",
    "    param.requires_grad = False  # Freezing all layers\n",
    "\n",
    "models[model_type].fc = nn.Linear(models[model_type].fc.in_features, len(classes))\n",
    "\n",
    "models[model_type] = models[model_type].to(device)\n",
    "\n",
    "params = []\n",
    "for param in models[model_type].parameters():\n",
    "    if param.requires_grad:\n",
    "        params.append(param)\n",
    "\n",
    "optimizers[model_type] = torch.optim.Adam(params, 1e-3, [0.9, 0.999])\n",
    "\n",
    "schedulers[model_type] = torch.optim.lr_scheduler.ExponentialLR(\n",
    "    optimizers[model_type],\n",
    "    gamma=0.8,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "criterions[model_type] = nn.CrossEntropyLoss()\n",
    "\n",
    "models[model_type], optimizers[model_type], losses[model_type], lrs[model_type], metrics[model_type] = learning_loop_with_lr(\n",
    "    model = models[model_type],\n",
    "    optimizer = optimizers[model_type],\n",
    "    train_loader = dataloaders['train'],\n",
    "    val_loader = dataloaders['val'],\n",
    "    criterion = criterions[model_type],\n",
    "    scheduler = schedulers[model_type],\n",
    "    epochs = 50,\n",
    "    min_lr = None,\n",
    "    metric_names = {'accuracy'},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "dups5i6ljj9njhy5222pb8"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_type = 'lr_cosine'\n",
    "\n",
    "models[model_type] = mds.resnet18(weights=resnet_weights, progress=True)\n",
    "\n",
    "for param in models[model_type].parameters():\n",
    "    param.requires_grad = False  # Freezing all layers\n",
    "\n",
    "models[model_type].fc = nn.Linear(models[model_type].fc.in_features, len(classes))\n",
    "\n",
    "models[model_type] = models[model_type].to(device)\n",
    "\n",
    "params = []\n",
    "for param in models[model_type].parameters():\n",
    "    if param.requires_grad:\n",
    "        params.append(param)\n",
    "\n",
    "optimizers[model_type] = torch.optim.Adam(params, 1e-3, [0.9, 0.999])\n",
    "\n",
    "schedulers[model_type] = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizers[model_type],\n",
    "    T_max=50,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "criterions[model_type] = nn.CrossEntropyLoss()\n",
    "\n",
    "models[model_type], optimizers[model_type], losses[model_type], lrs[model_type], metrics[model_type] = learning_loop_with_lr(\n",
    "    model = models[model_type],\n",
    "    optimizer = optimizers[model_type],\n",
    "    train_loader = dataloaders['train'],\n",
    "    val_loader = dataloaders['val'],\n",
    "    criterion = criterions[model_type],\n",
    "    scheduler = schedulers[model_type],\n",
    "    epochs = 50,\n",
    "    min_lr = None,\n",
    "    metric_names = {'accuracy'},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "28ihtp9hag8kawq44rkqfs"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_type = 'lr_constant'\n",
    "\n",
    "models[model_type] = mds.resnet18(weights=resnet_weights, progress=True)\n",
    "\n",
    "for param in models[model_type].parameters():\n",
    "    param.requires_grad = False  # Freezing all layers\n",
    "\n",
    "models[model_type].fc = nn.Linear(models[model_type].fc.in_features, len(classes))\n",
    "\n",
    "models[model_type] = models[model_type].to(device)\n",
    "\n",
    "params = []\n",
    "for param in models[model_type].parameters():\n",
    "    if param.requires_grad:\n",
    "        params.append(param)\n",
    "\n",
    "optimizers[model_type] = torch.optim.Adam(params, 1e-3, [0.9, 0.999])\n",
    "\n",
    "schedulers[model_type] = None\n",
    "\n",
    "criterions[model_type] = nn.CrossEntropyLoss()\n",
    "\n",
    "models[model_type], optimizers[model_type], losses[model_type], lrs[model_type], metrics[model_type] = learning_loop_with_lr(\n",
    "    model = models[model_type],\n",
    "    optimizer = optimizers[model_type],\n",
    "    train_loader = dataloaders['train'],\n",
    "    val_loader = dataloaders['val'],\n",
    "    criterion = criterions[model_type],\n",
    "    scheduler = schedulers[model_type],\n",
    "    epochs = 50,\n",
    "    min_lr = None,\n",
    "    metric_names = {'accuracy'},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "by6xyj30tma1t89dq9cyzv"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_type = 'lr_linear'\n",
    "\n",
    "models[model_type] = mds.resnet18(weights=resnet_weights, progress=True)\n",
    "\n",
    "for param in models[model_type].parameters():\n",
    "    param.requires_grad = False  # Freezing all layers\n",
    "\n",
    "models[model_type].fc = nn.Linear(models[model_type].fc.in_features, len(classes))\n",
    "\n",
    "models[model_type] = models[model_type].to(device)\n",
    "\n",
    "params = []\n",
    "for param in models[model_type].parameters():\n",
    "    if param.requires_grad:\n",
    "        params.append(param)\n",
    "\n",
    "optimizers[model_type] = torch.optim.Adam(params, 1e-3, [0.9, 0.999])\n",
    "\n",
    "schedulers[model_type] = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizers[model_type],\n",
    "    step_size=5,\n",
    "    gamma=0.25,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "criterions[model_type] = nn.CrossEntropyLoss()\n",
    "\n",
    "models[model_type], optimizers[model_type], losses[model_type], lrs[model_type], metrics[model_type] = learning_loop_with_lr(\n",
    "    model = models[model_type],\n",
    "    optimizer = optimizers[model_type],\n",
    "    train_loader = dataloaders['train'],\n",
    "    val_loader = dataloaders['val'],\n",
    "    criterion = criterions[model_type],\n",
    "    scheduler = schedulers[model_type],\n",
    "    epochs = 50,\n",
    "    min_lr = None,\n",
    "    metric_names = {'accuracy'},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "slx6g2e1qk12iiwr1jve9"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_type = 'lr_on_plateau'\n",
    "\n",
    "models[model_type] = mds.resnet18(weights=resnet_weights, progress=True)\n",
    "\n",
    "for param in models[model_type].parameters():\n",
    "    param.requires_grad = False  # Freezing all layers\n",
    "\n",
    "models[model_type].fc = nn.Linear(models[model_type].fc.in_features, len(classes))\n",
    "\n",
    "models[model_type] = models[model_type].to(device)\n",
    "\n",
    "params = []\n",
    "for param in models[model_type].parameters():\n",
    "    if param.requires_grad:\n",
    "        params.append(param)\n",
    "\n",
    "optimizers[model_type] = torch.optim.Adam(params, 1e-3, [0.9, 0.999])\n",
    "\n",
    "schedulers[model_type] = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizers[model_type],\n",
    "    mode='min',\n",
    "    factor=0.25,\n",
    "    patience=3,\n",
    "    threshold=1e-4,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "criterions[model_type] = nn.CrossEntropyLoss()\n",
    "\n",
    "models[model_type], optimizers[model_type], losses[model_type], lrs[model_type], metrics[model_type] = learning_loop_with_lr(\n",
    "    model = models[model_type],\n",
    "    optimizer = optimizers[model_type],\n",
    "    train_loader = dataloaders['train'],\n",
    "    val_loader = dataloaders['val'],\n",
    "    criterion = criterions[model_type],\n",
    "    scheduler = schedulers[model_type],\n",
    "    epochs = 50,\n",
    "    min_lr = None,\n",
    "    metric_names = {'accuracy'},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "9okpkwep4vmif61fm8fr1"
   },
   "source": [
    "**Сравним разные схемы**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "97iaa01aq376rh7j7hn9f"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(20, 10))\n",
    "\n",
    "cmap = plt.get_cmap('gnuplot2')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, 6)]\n",
    "\n",
    "k = 0\n",
    "for model_type in models:\n",
    "    if not model_type.startswith('lr'):\n",
    "        continue\n",
    "    ax[0].set_title('validation losses')\n",
    "    ax[0].plot(losses[model_type]['val'], '-', color=colors[k], label=model_type[3:])\n",
    "    ax[0].legend()\n",
    "    \n",
    "    ax[1].set_title('learning rate')\n",
    "    ax[1].plot(lrs[model_type], '-', color=colors[k], label=model_type[3:])\n",
    "    ax[1].legend()    \n",
    "    \n",
    "    ax[2].set_title('accuracy')\n",
    "    ax[2].plot(metrics[model_type]['accuracy'], '-', color=colors[k], label=model_type[3:])\n",
    "    ax[2].legend()\n",
    "    \n",
    "    k += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "d2w7r5ypi793tlireqdjcu"
   },
   "source": [
    "### 3.2 Warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "cpygtvocgpp075sj1x858hc"
   },
   "source": [
    "Позволяет избежать \"раннего переобучения\", которое может произойти при неудачном батче\n",
    "\n",
    "[Недавно появился torch ignite, но мы давайте напишем сами](https://pytorch.org/ignite/generated/ignite.handlers.param_scheduler.create_lr_scheduler_with_warmup.htmlhttps://pytorch.org/ignite/generated/ignite.handlers.param_scheduler.create_lr_scheduler_with_warmup.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "xafkrikwhbczhxqmkue2n"
   },
   "outputs": [],
   "source": [
    "# From https://www.tutorialexample.com/implement-warm-up-scheduler-in-pytorch-pytorch-example/\n",
    "class CosineWarmupScheduler(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, warmup, max_epoch, min_lr=1e-9):\n",
    "        self.warmup = warmup\n",
    "        self.max_num_iters = max_epoch\n",
    "        self.min_lr = min_lr\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch == 0:\n",
    "            return [self.min_lr]\n",
    "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
    "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
    "\n",
    "    def get_lr_factor(self, epoch):\n",
    "        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n",
    "        if epoch <= self.warmup:\n",
    "            lr_factor *= epoch * 1.0 / self.warmup\n",
    "        return lr_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "qsixoq0vk9r3y8zyb0ops"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_type = 'lr_warmup'\n",
    "\n",
    "models[model_type] = mds.resnet18(weights=resnet_weights, progress=True)\n",
    "\n",
    "for param in models[model_type].parameters():\n",
    "    param.requires_grad = False  # Freezing all layers\n",
    "\n",
    "models[model_type].fc = nn.Linear(models[model_type].fc.in_features, len(classes))\n",
    "\n",
    "models[model_type] = models[model_type].to(device)\n",
    "\n",
    "params = []\n",
    "for param in models[model_type].parameters():\n",
    "    if param.requires_grad:\n",
    "        params.append(param)\n",
    "\n",
    "optimizers[model_type] = torch.optim.Adam(params, 1e-3, [0.9, 0.999])\n",
    "\n",
    "schedulers[model_type] = CosineWarmupScheduler(\n",
    "    optimizer=optimizers[model_type],\n",
    "    warmup=5,\n",
    "    max_epoch=50,\n",
    "    min_lr=1e-7,\n",
    ")\n",
    "\n",
    "criterions[model_type] = nn.CrossEntropyLoss()\n",
    "\n",
    "models[model_type], optimizers[model_type], losses[model_type], lrs[model_type], metrics[model_type] = learning_loop_with_lr(\n",
    "    model = models[model_type],\n",
    "    optimizer = optimizers[model_type],\n",
    "    train_loader = dataloaders['train'],\n",
    "    val_loader = dataloaders['val'],\n",
    "    criterion = criterions[model_type],\n",
    "    scheduler = schedulers[model_type],\n",
    "    epochs = 50,\n",
    "    min_lr = None,\n",
    "    metric_names = {'accuracy'},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "37nrgfk4tfpr4fije8wdo"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(20, 10))\n",
    "\n",
    "cmap = plt.get_cmap('gnuplot2')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, 7)]\n",
    "\n",
    "k = 0\n",
    "for model_type in models:\n",
    "    if not model_type.startswith('lr'):\n",
    "        continue\n",
    "    ax[0].set_title('validation losses')\n",
    "    ax[0].plot(losses[model_type]['val'], '-', color=colors[k], label=model_type[3:])\n",
    "    ax[0].legend()\n",
    "    \n",
    "    ax[1].set_title('learning rate')\n",
    "    ax[1].plot(lrs[model_type], '-', color=colors[k], label=model_type[3:])\n",
    "    ax[1].legend()    \n",
    "    \n",
    "    ax[2].set_title('accuracy')\n",
    "    ax[2].plot(metrics[model_type]['accuracy'], '-', color=colors[k], label=model_type[3:])\n",
    "    ax[2].legend()\n",
    "    \n",
    "    k += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "t3l5l7x30vll399ancwjx"
   },
   "source": [
    "## 4. Борьба с переобучением"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "7uvfkh5trbcymw7oqxw1wh"
   },
   "source": [
    "### 4.1 Label smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "ct091wjd9niv4njavqr039"
   },
   "source": [
    "Хотим ограничить уверенность модели в своих ответах (слишком уверена - риск переобучения)\n",
    "\n",
    "Простое решение: ослабим таргет лейблы. Пусть теперь это будет не one-hot соответствующего класса, а ослабленный one-hot\n",
    "\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0] --> [0.1, 0.1, 0.1, 0.1, 0.91, 0.1, 0.1, 0.1, 0.1, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "60g7nufary8bkzqxbe4m5"
   },
   "outputs": [],
   "source": [
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1, weight = None):\n",
    "        \"\"\"if smoothing == 0, it's one-hot method\n",
    "           if 0 < smoothing < 1, it's smooth method\n",
    "        \"\"\"\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        assert 0 <= self.smoothing < 1\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "\n",
    "        if self.weight is not None:\n",
    "            pred = pred * self.weight.unsqueeze(0)   \n",
    "\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "23gxsq82xnfmzvck6ku2s"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_type = 'label_smooth'\n",
    "\n",
    "models[model_type] = mds.resnet18(weights=resnet_weights, progress=True)\n",
    "\n",
    "for param in models[model_type].parameters():\n",
    "    param.requires_grad = False  # Freezing all layers\n",
    "\n",
    "models[model_type].fc = nn.Linear(models[model_type].fc.in_features, len(classes))\n",
    "\n",
    "models[model_type] = models[model_type].to(device)\n",
    "\n",
    "params = []\n",
    "for param in models[model_type].parameters():\n",
    "    if param.requires_grad:\n",
    "        params.append(param)\n",
    "\n",
    "optimizers[model_type] = torch.optim.Adam(params, 1e-3, [0.9, 0.999])\n",
    "\n",
    "schedulers[model_type] = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizers[model_type],\n",
    "    T_max=50,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "criterions[model_type] = LabelSmoothingCrossEntropy(\n",
    "    classes = len(classes),\n",
    "    smoothing = 0.3,\n",
    "    dim = -1,\n",
    "    weight = None,\n",
    ")\n",
    "\n",
    "models[model_type], optimizers[model_type], losses[model_type], lrs[model_type], metrics[model_type] = learning_loop_with_lr(\n",
    "    model = models[model_type],\n",
    "    optimizer = optimizers[model_type],\n",
    "    train_loader = dataloaders['train'],\n",
    "    val_loader = dataloaders['val'],\n",
    "    criterion = criterions[model_type],\n",
    "    scheduler = schedulers[model_type],\n",
    "    epochs = 50,\n",
    "    min_lr = None,\n",
    "    metric_names = {'accuracy'},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "a3fhqpj248qgpkd1smh6"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(20, 10))\n",
    "\n",
    "cmap = plt.get_cmap('gnuplot2')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, 7)]\n",
    "\n",
    "k = 0\n",
    "for model_type in {'lr_cosine', 'label_smooth'}:\n",
    "    ax[0].set_title('train losses')\n",
    "    ax[0].plot(losses[model_type]['train'], '-', color=colors[k], label=model_type[3:])\n",
    "    ax[0].legend()  \n",
    "    \n",
    "    ax[1].set_title('validation losses')\n",
    "    ax[1].plot(losses[model_type]['val'], '-', color=colors[k], label=model_type[3:])\n",
    "    ax[1].legend()\n",
    "    \n",
    "    ax[2].set_title('accuracy')\n",
    "    ax[2].plot(metrics[model_type]['accuracy'], '-', color=colors[k], label=model_type[3:])\n",
    "    ax[2].legend()\n",
    "    \n",
    "    k += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "2yohwym7x0ayyk30uevwar"
   },
   "source": [
    "### 4.2 Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "svocnvk5fhm0tf2klwhpmzf"
   },
   "source": [
    "$$\\text{basic softmax}: \\qquad b_i = \\frac{\\exp(a_i)}{\\sum_i \\exp(a_i)}$$\n",
    "$$\\text{softmax with temperature}: \\qquad b_i = \\frac{\\exp(\\frac{a_i}{t})}{\\sum_i \\exp(\\frac{a_i}{t})}$$\n",
    "\n",
    "По сути мы делаем то же что делали в label smoothing, но теперь мы это делаем не на уровне правильных ответов, а прямо на уровне предсказаний\n",
    "\n",
    "А так же t - настраиваемый параметр, обычно хотим начинать с большой температуры и постепенно снижать её"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "e937c154-7b7b-4237-a987-f8a696322692",
  "notebookPath": "Sem3 - DL tricks/sem3_dl_tricks_master-Copy1.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
